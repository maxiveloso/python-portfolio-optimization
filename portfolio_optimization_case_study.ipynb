{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd13b02",
   "metadata": {},
   "source": [
    "# Portfolio Optimization - Case Study\n",
    "\n",
    "## Business Overview\n",
    "**Client**: Investment Management Firm  \n",
    "**Objective**: Build optimized portfolios with superior risk-adjusted returns vs benchmark  \n",
    "**Investment Horizon**: Medium to long-term (2+ years)\n",
    "\n",
    "**Comparison Framework**: Equal-Weight vs sharpe-Optimized vs Benchmark (QQQ)\n",
    "\n",
    "**Key Constraints**: \n",
    "- No short selling allowed\n",
    "- No leverage permitted  \n",
    "- Monthly rebalancing frequency\n",
    "\n",
    "## Expected Deliverables\n",
    "- Executive KPI dashboard with key metrics (CAGR, sharpe, MDD, Calmar)\n",
    "- Risk-return visualization with efficient frontier\n",
    "- Stress test analysis across market regimes\n",
    "- Reproducible results with documented assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711414-a2d3-4470-89fc-fadc5964f6cd",
   "metadata": {},
   "source": [
    "# Configuration, Global Parameters, Logging & Reproducibility Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f594a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION & SETUP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "from itertools import combinations\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "# Configure display and warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b42433-1755-4d29-a7d2-6e50a34f11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PARAMETERS - EDIT THESE FOR DIFFERENT SCENARIOS\n",
    "\n",
    "# Get the directory of the current notebook and navigate to the data folder\n",
    "try:\n",
    "    # Use __file__ when the notebook is run as a script (e.g., in a Binder or Colab environment)\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    DATA_FOLDER = os.path.join(NOTEBOOK_DIR, '..', 'data')\n",
    "except NameError:\n",
    "    # Fallback for local Jupyter environment\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.getcwd())\n",
    "    DATA_FOLDER = os.path.join(NOTEBOOK_DIR, 'data')\n",
    "\n",
    "\n",
    "# Stock universe (add more tickers as needed)\n",
    "STOCK_UNIVERSE = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX', 'AMD', 'CRM']\n",
    "N_STOCKS_AUTO = 4  # Number of stocks to select automatically\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "MODE = 'snapshot'  # 'snapshot' or 'live'\n",
    "\n",
    "# Risk Management\n",
    "MAX_WEIGHT = 0.40  # Maximum allocation per asset (40%)\n",
    "REBALANCE_FREQUENCY = 'M'  # Monthly rebalancing\n",
    "COST_PER_SIDE = 0.001  # 10 bps transaction cost\n",
    "\n",
    "# Benchmark & Analysis\n",
    "BENCHMARK = 'QQQ'  # NASDAQ-100 ETF\n",
    "TRAIN_END_DATE = '2023-12-31'  # Train/test split\n",
    "MIN_DAYS = 500  # Minimum days required for analysis\n",
    "\n",
    "# Monte Carlo & Reproducibility  \n",
    "N_SCENARIOS = 20000  # Number of portfolio scenarios\n",
    "RNG_SEED = 42  # For reproducible results\n",
    "PORTFOLIO_VALUE = 10000  # Initial portfolio value ($10k)\n",
    "\n",
    "# Walk-Forward Analysis Parameters\n",
    "TRAIN_WINDOW = 252  # Training window in days (1 year)\n",
    "TEST_WINDOW = 63   # Test window in days (3 months)\n",
    "RISK_FREE_RATE = 0.02  # Risk-free rate for Sharpe calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f63dd32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 17:05:39,039 - INFO - === PORTFOLIO OPTIMIZATION ENHANCED PARAMETERS ===\n",
      "2025-08-22 17:05:39,040 - INFO - Mode: snapshot\n",
      "2025-08-22 17:05:39,041 - INFO - Stock Universe: ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX', 'AMD', 'CRM']\n",
      "2025-08-22 17:05:39,042 - INFO - Auto-selection count: 4\n",
      "2025-08-22 17:05:39,042 - INFO - Date Range: 2020-01-01 to 2024-12-31\n",
      "2025-08-22 17:05:39,043 - INFO - Benchmark: QQQ\n",
      "2025-08-22 17:05:39,044 - INFO - Max Weight per Asset: 40.0%\n",
      "2025-08-22 17:05:39,044 - INFO - Transaction Cost: 10.0 bps per side\n",
      "2025-08-22 17:05:39,045 - INFO - Monte Carlo Scenarios: 20,000\n",
      "2025-08-22 17:05:39,045 - INFO - Random Seed: 42\n",
      "2025-08-22 17:05:39,046 - INFO - Portfolio Value: $10,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n",
      "📊 Analysis Mode: SNAPSHOT\n",
      "📈 Stock Universe: 10 stocks\n",
      "📅 Period: 2020-01-01 to 2024-12-31\n",
      "🎯 Benchmark: QQQ\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RNG_SEED)\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Utility functions for reproducibility\n",
    "def _rng(seed=None):\n",
    "    \"\"\"Return a reproducible NumPy Generator.\"\"\"\n",
    "    return np.random.default_rng(RNG_SEED if seed is None else seed)\n",
    "\n",
    "def _annualize_return(mean_daily):\n",
    "    return mean_daily * 252.0\n",
    "\n",
    "def _annualize_vol(std_daily):\n",
    "    return std_daily * np.sqrt(252.0)\n",
    "\n",
    "def _sharpe(mean, std, rf=0.0, eps=1e-12):\n",
    "    return (mean - rf) / max(std, eps)\n",
    "\n",
    "def _max_drawdown(nav: pd.Series) -> float:\n",
    "    roll_max = nav.cummax()\n",
    "    dd = nav / roll_max - 1.0\n",
    "    return float(dd.min()) if len(dd) else 0.0\n",
    "\n",
    "def _normalize_weights(w: np.ndarray, eps=1e-12):\n",
    "    s = np.sum(w)\n",
    "    if s <= eps:\n",
    "        return np.zeros_like(w)\n",
    "    return w / s\n",
    "\n",
    "def _to_series_weights(w: np.ndarray, tickers):\n",
    "    return pd.Series(w, index=list(tickers), dtype=float)\n",
    "\n",
    "# Log key parameters\n",
    "logger.info(\"=== PORTFOLIO OPTIMIZATION ENHANCED PARAMETERS ===\")\n",
    "logger.info(f\"Mode: {MODE}\")\n",
    "logger.info(f\"Stock Universe: {STOCK_UNIVERSE}\")\n",
    "logger.info(f\"Auto-selection count: {N_STOCKS_AUTO}\")\n",
    "logger.info(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "logger.info(f\"Benchmark: {BENCHMARK}\")\n",
    "logger.info(f\"Max Weight per Asset: {MAX_WEIGHT*100}%\")\n",
    "logger.info(f\"Transaction Cost: {COST_PER_SIDE*10000} bps per side\")\n",
    "logger.info(f\"Monte Carlo Scenarios: {N_SCENARIOS:,}\")\n",
    "logger.info(f\"Random Seed: {RNG_SEED}\")\n",
    "logger.info(f\"Portfolio Value: ${PORTFOLIO_VALUE:,}\")\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")\n",
    "print(f\"📊 Analysis Mode: {MODE.upper()}\")\n",
    "print(f\"📈 Stock Universe: {len(STOCK_UNIVERSE)} stocks\")\n",
    "print(f\"📅 Period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"🎯 Benchmark: {BENCHMARK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6ff72",
   "metadata": {},
   "source": [
    "## DATA INGESTION & VALIDATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5223eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA INGESTION & VALIDATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb16fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices_from_csv(tickers, start_date, end_date, data_folder='./', strict=True):\n",
    "    \"\"\"\n",
    "    Load stock prices from CSV files with comprehensive validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tickers : list\n",
    "        List of stock ticker symbols\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str  \n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    data_folder : str\n",
    "        Path to folder containing CSV files\n",
    "    strict : bool\n",
    "        If True, raise errors for data issues; if False, issue warnings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aligned stock prices (Close Price only)\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading price data for {len(tickers)} tickers...\")\n",
    "    \n",
    "    stock_data = {}\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Construct file path\n",
    "            file_path = os.path.join(data_folder, f\"{ticker}.csv\")\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "                failed_tickers.append(ticker)\n",
    "                continue\n",
    "            \n",
    "            # Load CSV with date parsing\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "            \n",
    "            # Filter date range\n",
    "            df = df.loc[start_date:end_date]\n",
    "            \n",
    "            if df.empty:\n",
    "                logger.warning(f\"No data for {ticker} in specified date range\")\n",
    "                failed_tickers.append(ticker)\n",
    "                continue\n",
    "            \n",
    "            # Store only Close Price\n",
    "            stock_data[ticker] = df['Close Price']\n",
    "            logger.info(f\"✅ {ticker}: {len(df)} records loaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {ticker}: {str(e)}\")\n",
    "            failed_tickers.append(ticker)\n",
    "    \n",
    "    if not stock_data:\n",
    "        raise ValueError(\"No valid stock data loaded!\")\n",
    "    \n",
    "    # Create aligned DataFrame\n",
    "    stock_adj_close = pd.DataFrame(stock_data)\n",
    "    \n",
    "    # Data validation\n",
    "    stock_adj_close = validate_stock_data(stock_adj_close, tickers, failed_tickers, strict)\n",
    "    \n",
    "    logger.info(f\"📊 Final dataset: {len(stock_adj_close.columns)} stocks, {len(stock_adj_close)} days\")\n",
    "    return stock_adj_close\n",
    "\n",
    "def validate_stock_data(df, original_tickers, failed_tickers, strict=True):\n",
    "    \"\"\"\n",
    "    Comprehensive data validation and cleaning\n",
    "    \"\"\"\n",
    "    logger.info(\"Performing data validation...\")\n",
    "    \n",
    "    # Report failed tickers\n",
    "    if failed_tickers:\n",
    "        logger.warning(f\"Failed to load: {failed_tickers}\")\n",
    "    \n",
    "    # Check minimum data requirements\n",
    "    if len(df) < MIN_DAYS:\n",
    "        message = f\"Dataset has only {len(df)} days (minimum: {MIN_DAYS})\"\n",
    "        if strict:\n",
    "            raise ValueError(message)\n",
    "        else:\n",
    "            logger.warning(message)\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.any():\n",
    "        logger.warning(\"Missing data detected:\")\n",
    "        for ticker, missing_count in missing_data[missing_data > 0].items():\n",
    "            logger.warning(f\"  {ticker}: {missing_count} missing values\")\n",
    "        \n",
    "        # Forward fill missing values\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        logger.info(\"Missing values filled using forward/backward fill\")\n",
    "    \n",
    "    # Check for duplicated dates\n",
    "    if df.index.duplicated().any():\n",
    "        logger.warning(\"Duplicate dates found - removing duplicates\")\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Remove stocks with insufficient data\n",
    "    min_valid_ratio = 0.95  # Require 95% valid data\n",
    "    for ticker in df.columns.copy():\n",
    "        valid_ratio = df[ticker].notna().sum() / len(df)\n",
    "        if valid_ratio < min_valid_ratio:\n",
    "            logger.warning(f\"Removing {ticker}: only {valid_ratio:.1%} valid data\")\n",
    "            df = df.drop(columns=[ticker])\n",
    "    \n",
    "    # Check for extreme outliers and constant series\n",
    "    returns = df.pct_change().dropna()\n",
    "    for ticker in returns.columns.copy():\n",
    "        # Check for constant prices (no variation)\n",
    "        if returns[ticker].std() < 1e-6:\n",
    "            logger.warning(f\"Removing {ticker}: constant price series\")\n",
    "            df = df.drop(columns=[ticker])\n",
    "            continue\n",
    "        \n",
    "        # Check for extreme movements (>50% daily change)\n",
    "        extreme_moves = (abs(returns[ticker]) > 0.5).sum()\n",
    "        if extreme_moves > 0:\n",
    "            logger.warning(f\"{ticker}: {extreme_moves} extreme daily moves (>50%)\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid stocks remaining after validation!\")\n",
    "    \n",
    "    logger.info(f\"✅ Validation complete: {len(df.columns)} stocks, {len(df)} days\")\n",
    "    return df\n",
    "\n",
    "def compute_returns(prices_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute daily returns from prices (pct_change), drop first NaNs.\"\"\"\n",
    "    returns = prices_df.pct_change().dropna(how='all')\n",
    "    return returns.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced_kpis",
   "metadata": {},
   "source": [
    "## ENHANCED KPI CALCULATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced_kpis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED KPI CALCULATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comprehensive_kpis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_kpis(returns: pd.Series, benchmark_returns: pd.Series = None, rf: float = RISK_FREE_RATE) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive KPIs including advanced risk metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Portfolio returns series\n",
    "    benchmark_returns : pd.Series, optional\n",
    "        Benchmark returns for alpha/beta calculation\n",
    "    rf : float\n",
    "        Risk-free rate (annualized)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with comprehensive KPIs\n",
    "    \"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return {\"ann_return\": 0.0, \"ann_vol\": 0.0, \"sharpe\": 0.0, \"max_dd\": 0.0,\n",
    "                \"sortino\": 0.0, \"var_95\": 0.0, \"cvar_95\": 0.0, \"alpha\": 0.0, \"beta\": 0.0, \"info_ratio\": 0.0}\n",
    "    \n",
    "    # Basic metrics\n",
    "    mu_d = float(returns.mean())\n",
    "    sd_d = float(returns.std(ddof=0))\n",
    "    ann_r = _annualize_return(mu_d)\n",
    "    ann_v = _annualize_vol(sd_d)\n",
    "    sr = _sharpe(ann_r, ann_v, rf)\n",
    "    \n",
    "    # Maximum Drawdown\n",
    "    nav = (1 + returns).cumprod()\n",
    "    mdd = _max_drawdown(nav)\n",
    "    \n",
    "    # Sortino Ratio (downside deviation)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_std = float(downside_returns.std(ddof=0)) if len(downside_returns) > 0 else 0.0\n",
    "    sortino = _sharpe(ann_r, _annualize_vol(downside_std), rf) if downside_std > 0 else 0.0\n",
    "    \n",
    "    # Value at Risk (VaR) and Conditional VaR (CVaR) at 95% confidence\n",
    "    var_95 = float(np.percentile(returns, 5)) * np.sqrt(252)  # Annualized\n",
    "    cvar_95 = float(returns[returns <= np.percentile(returns, 5)].mean()) * np.sqrt(252)  # Annualized\n",
    "    \n",
    "    # Alpha, Beta, and Information Ratio vs benchmark\n",
    "    alpha, beta, info_ratio = 0.0, 0.0, 0.0\n",
    "    if benchmark_returns is not None and len(benchmark_returns) > 0:\n",
    "        # Align returns\n",
    "        aligned_idx = returns.index.intersection(benchmark_returns.index)\n",
    "        if len(aligned_idx) > 10:  # Need sufficient data\n",
    "            port_aligned = returns.reindex(aligned_idx).fillna(0.0)\n",
    "            bench_aligned = benchmark_returns.reindex(aligned_idx).fillna(0.0)\n",
    "            \n",
    "            # Beta calculation\n",
    "            if bench_aligned.std() > 0:\n",
    "                beta = float(np.cov(port_aligned, bench_aligned)[0, 1] / np.var(bench_aligned))\n",
    "            \n",
    "            # Alpha calculation (annualized)\n",
    "            bench_ann_return = _annualize_return(bench_aligned.mean())\n",
    "            alpha = ann_r - (rf + beta * (bench_ann_return - rf))\n",
    "            \n",
    "            # Information Ratio\n",
    "            active_returns = port_aligned - bench_aligned\n",
    "            tracking_error = _annualize_vol(active_returns.std(ddof=0))\n",
    "            if tracking_error > 0:\n",
    "                info_ratio = _annualize_return(active_returns.mean()) / tracking_error\n",
    "    \n",
    "    return {\n",
    "        \"ann_return\": ann_r,\n",
    "        \"ann_vol\": ann_v,\n",
    "        \"sharpe\": sr,\n",
    "        \"max_dd\": mdd,\n",
    "        \"sortino\": sortino,\n",
    "        \"var_95\": var_95,\n",
    "        \"cvar_95\": cvar_95,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta,\n",
    "        \"info_ratio\": info_ratio,\n",
    "        \"mu_d\": mu_d,\n",
    "        \"vol_d\": sd_d\n",
    "    }\n",
    "\n",
    "def compute_kpis(returns: pd.Series, rf: float = 0.0) -> dict:\n",
    "    \"\"\"Compute basic KPIs from a return series (backward compatibility).\"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return {\"ann_return\": 0.0, \"ann_vol\": 0.0, \"sharpe\": 0.0, \"max_dd\": 0.0}\n",
    "    mu_d = float(returns.mean())\n",
    "    sd_d = float(returns.std(ddof=0))\n",
    "    ann_r = _annualize_return(mu_d)\n",
    "    ann_v = _annualize_vol(sd_d)\n",
    "    sr = _sharpe(ann_r, ann_v, rf)\n",
    "    nav = (1 + returns).cumprod()\n",
    "    mdd = _max_drawdown(nav)\n",
    "    return {\"ann_return\": ann_r, \"ann_vol\": ann_v, \"sharpe\": sr, \"max_dd\": mdd}\n",
    "\n",
    "def kpis_vs_benchmark(port_ret: pd.Series, bench_ret: pd.Series) -> dict:\n",
    "    \"\"\"Compute correlation, tracking error, and active return vs benchmark.\"\"\"\n",
    "    idx = port_ret.index.intersection(bench_ret.index)\n",
    "    pr = port_ret.reindex(idx).fillna(0.0)\n",
    "    br = bench_ret.reindex(idx).fillna(0.0)\n",
    "    corr = float(np.corrcoef(pr, br)[0,1]) if pr.std() > 0 and br.std() > 0 else 0.0\n",
    "    active = pr - br\n",
    "    te = float(active.std(ddof=0) * np.sqrt(252.0))\n",
    "    ar = float((_annualize_return(pr.mean()) - _annualize_return(br.mean())))\n",
    "    return {\"corr\": corr, \"tracking_error\": te, \"active_return\": ar}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portfolio_optimization",
   "metadata": {},
   "source": [
    "## PORTFOLIO OPTIMIZATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portfolio_optimization_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PORTFOLIO OPTIMIZATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "optimization_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_offdiag_correlation(returns_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Average off-diagonal correlation among assets.\"\"\"\n",
    "    if returns_df.shape[1] <= 1:\n",
    "        return 0.0\n",
    "    corr = returns_df.corr().values\n",
    "    n = corr.shape[0]\n",
    "    return float((corr.sum() - np.trace(corr)) / (n * (n - 1)))\n",
    "\n",
    "def portfolio_stats_from_weights(returns_df: pd.DataFrame, weights: np.ndarray, rf: float = 0.0) -> dict:\n",
    "    \"\"\"Compute mean, vol, sharpe given asset returns and weights.\"\"\"\n",
    "    w = np.asarray(weights).reshape(-1)\n",
    "    mu = returns_df.mean().values\n",
    "    cov = returns_df.cov().values\n",
    "    pmu_d = float(np.dot(w, mu))\n",
    "    pvol_d = float(np.sqrt(max(np.dot(w, cov @ w), 0.0)))\n",
    "    ann_r = _annualize_return(pmu_d)\n",
    "    ann_v = _annualize_vol(pvol_d)\n",
    "    sr = _sharpe(ann_r, ann_v, rf)\n",
    "    return {\"ann_return\": ann_r, \"ann_vol\": ann_v, \"sharpe\": sr, \"mu_d\": pmu_d, \"vol_d\": pvol_d}\n",
    "\n",
    "def optimize_weights(\n",
    "    returns_df: pd.DataFrame,\n",
    "    objective: str = 'max_sharpe',\n",
    "    bounds: tuple = (0.0, MAX_WEIGHT),\n",
    "    sum_to: float = 1.0,\n",
    "    method: str = 'scipy',\n",
    "    turnover_penalty: float = 0.0,\n",
    "    prev_weights: np.ndarray = None,\n",
    "    risk_free: float = RISK_FREE_RATE,\n",
    "    seed: int = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Optimize portfolio weights under constraints with enhanced objectives\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Asset returns DataFrame\n",
    "    objective : str\n",
    "        'max_sharpe', 'min_var', or 'risk_parity'\n",
    "    bounds : tuple\n",
    "        (lower_bound, upper_bound) for weights\n",
    "    sum_to : float\n",
    "        Target sum of weights (usually 1.0)\n",
    "    method : str\n",
    "        'scipy' or 'mc' (Monte Carlo)\n",
    "    turnover_penalty : float\n",
    "        Penalty for turnover (L1 norm of weight changes)\n",
    "    prev_weights : np.ndarray\n",
    "        Previous weights for turnover calculation\n",
    "    risk_free : float\n",
    "        Risk-free rate\n",
    "    seed : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with optimized weights, KPIs, and method used\n",
    "    \"\"\"\n",
    "    assets = list(returns_df.columns)\n",
    "    n = len(assets)\n",
    "    lb, ub = bounds\n",
    "    lb = max(0.0, float(lb))\n",
    "    ub = float(ub)\n",
    "    if ub <= 0:\n",
    "        raise ValueError(\"Upper bound must be > 0\")\n",
    "\n",
    "    mu = returns_df.mean().values\n",
    "    cov = returns_df.cov().values\n",
    "    x0 = np.full(n, sum_to / n, dtype=float)\n",
    "    x0 = np.clip(x0, lb, ub)\n",
    "    x0 = _normalize_weights(x0) * sum_to\n",
    "    prev_w = np.zeros(n) if prev_weights is None else np.asarray(prev_weights).reshape(-1)\n",
    "\n",
    "    def obj_sharpe(w):\n",
    "        w = np.asarray(w)\n",
    "        pmu_d = np.dot(w, mu)\n",
    "        pvar = max(np.dot(w, cov @ w), 0.0)\n",
    "        pstd_d = np.sqrt(pvar)\n",
    "        ann_r = _annualize_return(pmu_d)\n",
    "        ann_v = _annualize_vol(pstd_d)\n",
    "        sharpe = _sharpe(ann_r, ann_v, risk_free)\n",
    "        pen = turnover_penalty * np.sum(np.abs(w - prev_w)) if turnover_penalty > 0 else 0.0\n",
    "        return -sharpe + pen\n",
    "\n",
    "    def obj_minvar(w):\n",
    "        w = np.asarray(w)\n",
    "        pen = turnover_penalty * np.sum(np.abs(w - prev_w)) if turnover_penalty > 0 else 0.0\n",
    "        return max(np.dot(w, cov @ w), 0.0) + pen\n",
    "\n",
    "    def obj_risk_parity(w):\n",
    "        \"\"\"Risk Parity objective: minimize sum of squared deviations from equal risk contribution\"\"\"\n",
    "        w = np.asarray(w)\n",
    "        covw = cov @ w\n",
    "        rc = w * covw  # risk contributions using variance\n",
    "        total_var = max(float(w @ covw), 1e-12)\n",
    "        target = total_var / n\n",
    "        pen = turnover_penalty * np.sum(np.abs(w - prev_w)) if turnover_penalty > 0 else 0.0\n",
    "        return float(np.sum((rc - target) ** 2)) + pen\n",
    "\n",
    "    def project_bounds(w):\n",
    "        w = np.clip(w, lb, ub)\n",
    "        if w.sum() <= 0:\n",
    "            return np.full_like(w, sum_to / len(w))\n",
    "        return w / w.sum() * sum_to\n",
    "\n",
    "    # Try SciPy optimization first\n",
    "    if method == 'scipy':\n",
    "        try:\n",
    "            cons = (\n",
    "                {'type': 'eq', 'fun': lambda w: np.sum(w) - sum_to},\n",
    "            )\n",
    "            bnds = tuple((lb, ub) for _ in range(n))\n",
    "            \n",
    "            if objective == 'max_sharpe':\n",
    "                fun = obj_sharpe\n",
    "            elif objective == 'min_var':\n",
    "                fun = obj_minvar\n",
    "            elif objective == 'risk_parity':\n",
    "                fun = obj_risk_parity\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown objective: {objective}\")\n",
    "            \n",
    "            res = minimize(fun, x0, method='SLSQP', bounds=bnds, constraints=cons, \n",
    "                         options={'maxiter': 200, 'ftol': 1e-9})\n",
    "            \n",
    "            if res.success:\n",
    "                w_opt = project_bounds(res.x)\n",
    "                stats = portfolio_stats_from_weights(returns_df, w_opt, rf=risk_free)\n",
    "                return {\n",
    "                    'weights': _to_series_weights(w_opt, assets),\n",
    "                    'kpis': stats,\n",
    "                    'method_used': 'scipy',\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SciPy optimization failed: {e}, falling back to Monte Carlo\")\n",
    "    \n",
    "    # Monte Carlo fallback\n",
    "    logger.info(\"Using Monte Carlo optimization\")\n",
    "    rng_local = _rng(seed)\n",
    "    W = rng_local.dirichlet(alpha=np.ones(n), size=N_SCENARIOS)\n",
    "    mask = (W <= ub + 1e-12).all(axis=1)\n",
    "    Wf = W[mask]\n",
    "    \n",
    "    if Wf.shape[0] < max(1000, n * 200):\n",
    "        Wc = np.clip(W, lb, ub)\n",
    "        Wc = (Wc / Wc.sum(axis=1, keepdims=True)) * sum_to\n",
    "        Wf = Wc\n",
    "    else:\n",
    "        Wf = (Wf / Wf.sum(axis=1, keepdims=True)) * sum_to\n",
    "\n",
    "    if objective == 'max_sharpe':\n",
    "        pmu_d = Wf @ mu\n",
    "        pvar = np.einsum('ij,jk,ik->i', Wf, cov, Wf)\n",
    "        pstd_d = np.sqrt(np.maximum(pvar, 0.0))\n",
    "        ann_r = _annualize_return(pmu_d)\n",
    "        ann_v = _annualize_vol(pstd_d)\n",
    "        sharpe = (ann_r - risk_free) / np.maximum(ann_v, 1e-12)\n",
    "        if turnover_penalty > 0:\n",
    "            pen = turnover_penalty * np.sum(np.abs(Wf - prev_w.reshape(1, -1)), axis=1)\n",
    "            sharpe = sharpe - pen\n",
    "        idx = int(np.argmax(sharpe))\n",
    "        w_opt = Wf[idx]\n",
    "    elif objective == 'min_var':\n",
    "        pvar = np.einsum('ij,jk,ik->i', Wf, cov, Wf)\n",
    "        if turnover_penalty > 0:\n",
    "            pen = turnover_penalty * np.sum(np.abs(Wf - prev_w.reshape(1, -1)), axis=1)\n",
    "            score = pvar + pen\n",
    "            idx = int(np.argmin(score))\n",
    "        else:\n",
    "            idx = int(np.argmin(pvar))\n",
    "        w_opt = Wf[idx]\n",
    "    elif objective == 'risk_parity':\n",
    "        covW = Wf @ cov  # (m, n)\n",
    "        rc = Wf * covW   # (m, n)\n",
    "        total_var = np.sum(rc, axis=1)\n",
    "        target = (total_var / n).reshape(-1, 1)\n",
    "        score = np.sum((rc - target) ** 2, axis=1)\n",
    "        if turnover_penalty > 0:\n",
    "            score = score + turnover_penalty * np.sum(np.abs(Wf - prev_w.reshape(1, -1)), axis=1)\n",
    "        idx = int(np.argmin(score))\n",
    "        w_opt = Wf[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown objective: {objective}\")\n",
    "\n",
    "    w_opt = project_bounds(w_opt)\n",
    "    stats = portfolio_stats_from_weights(returns_df, w_opt, rf=risk_free)\n",
    "    return {\n",
    "        'weights': _to_series_weights(w_opt, assets),\n",
    "        'kpis': stats,\n",
    "        'method_used': 'mc',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock_selection",
   "metadata": {},
   "source": [
    "## STOCK SELECTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock_selection_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCK SELECTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "selection_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimal_stocks(\n",
    "    prices_df: pd.DataFrame,\n",
    "    N_stocks: int,\n",
    "    lookback_days: int,\n",
    "    method: str = 'greedy',\n",
    "    seed: int = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Select N_stocks from the universe using advanced selection methodology\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Price data for all assets\n",
    "    N_stocks : int\n",
    "        Number of stocks to select\n",
    "    lookback_days : int\n",
    "        Number of days to look back for analysis\n",
    "    method : str\n",
    "        'exhaustive' for small universes, 'greedy' for larger ones\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with selected tickers, analysis DataFrame, and best weights\n",
    "    \"\"\"\n",
    "    assert N_stocks >= 1, \"N_stocks must be >= 1\"\n",
    "    tickers = list(prices_df.columns)\n",
    "    end_idx = prices_df.index[-1]\n",
    "    start_idx = prices_df.index[max(0, len(prices_df) - lookback_days)]\n",
    "    px_lb = prices_df.loc[start_idx:end_idx]\n",
    "    rets_lb = compute_returns(px_lb)\n",
    "    combos_rows = []\n",
    "\n",
    "    if len(tickers) <= 10 and method == 'exhaustive':\n",
    "        logger.info(f\"Using exhaustive search for {len(tickers)} assets\")\n",
    "        best = None\n",
    "        for combo in combinations(tickers, N_stocks):\n",
    "            sub = rets_lb[list(combo)].dropna()\n",
    "            if sub.shape[0] < 2:\n",
    "                continue\n",
    "            opt = optimize_weights(sub, objective='max_sharpe', bounds=(0.0, MAX_WEIGHT), \n",
    "                                 sum_to=1.0, method='scipy', seed=seed)\n",
    "            k = opt['kpis']\n",
    "            avg_corr = average_offdiag_correlation(sub)\n",
    "            score = k['sharpe']\n",
    "            combos_rows.append({\n",
    "                'combo': combo,\n",
    "                'sharpe': k['sharpe'],\n",
    "                'ann_return': k['ann_return'],\n",
    "                'ann_vol': k['ann_vol'],\n",
    "                'avg_corr': avg_corr,\n",
    "                'score': score,\n",
    "                'weights': opt['weights'],\n",
    "            })\n",
    "            if best is None or score > best['score']:\n",
    "                best = combos_rows[-1]\n",
    "        combos_df = pd.DataFrame(combos_rows)\n",
    "        sel = list(best['combo']) if best else tickers[:N_stocks]\n",
    "        best_weights = best['weights'] if best else pd.Series(np.full(N_stocks, 1.0 / N_stocks), index=sel)\n",
    "        return {'selected_tickers': sel, 'combos_df': combos_df, 'best_weights': best_weights}\n",
    "\n",
    "    # Greedy selection for larger universes\n",
    "    logger.info(f\"Using greedy search for {len(tickers)} assets\")\n",
    "    selected = []\n",
    "    remaining = set(tickers)\n",
    "    best_weights = None\n",
    "    \n",
    "    while len(selected) < N_stocks and remaining:\n",
    "        best_local = None\n",
    "        for cand in list(remaining):\n",
    "            trial = selected + [cand]\n",
    "            sub = rets_lb[trial].dropna()\n",
    "            if sub.shape[0] < 2:\n",
    "                continue\n",
    "            opt = optimize_weights(sub, objective='max_sharpe', bounds=(0.0, MAX_WEIGHT), \n",
    "                                 sum_to=1.0, method='scipy', seed=seed)\n",
    "            avg_corr = average_offdiag_correlation(sub)\n",
    "            score = opt['kpis']['sharpe'] * (1.0 - avg_corr)  # Penalize high correlation\n",
    "            row = {\n",
    "                'combo': tuple(trial),\n",
    "                'sharpe': opt['kpis']['sharpe'],\n",
    "                'ann_return': opt['kpis']['ann_return'],\n",
    "                'ann_vol': opt['kpis']['ann_vol'],\n",
    "                'avg_corr': avg_corr,\n",
    "                'score': score,\n",
    "                'weights': opt['weights'],\n",
    "            }\n",
    "            if best_local is None or score > best_local['score']:\n",
    "                best_local = row\n",
    "        \n",
    "        if best_local is None:\n",
    "            break\n",
    "        \n",
    "        selected = list(best_local['combo'])\n",
    "        best_weights = best_local['weights']\n",
    "        remaining = set(tickers) - set(selected)\n",
    "        combos_rows.append(best_local)\n",
    "    \n",
    "    combos_df = pd.DataFrame(combos_rows)\n",
    "    if len(selected) > N_stocks:\n",
    "        selected = selected[:N_stocks]\n",
    "        best_weights = best_weights[selected]\n",
    "        best_weights = best_weights / best_weights.sum()\n",
    "    \n",
    "    logger.info(f\"Selected {len(selected)} stocks: {selected}\")\n",
    "    return {'selected_tickers': selected, 'combos_df': combos_df, 'best_weights': best_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation_functions",
   "metadata": {},
   "source": [
    "## SIMULATION & REBALANCING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulation_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATION & REBALANCING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "simulation_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weights_schedule(start_date: pd.Timestamp, end_date: pd.Timestamp, weights: pd.Series, freq: str = 'M') -> pd.DataFrame:\n",
    "    \"\"\"Create a schedule with constant weights at a given frequency between dates (inclusive).\"\"\"\n",
    "    idx = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    if len(idx) == 0:\n",
    "        idx = pd.DatetimeIndex([start_date])\n",
    "    ws = pd.DataFrame(index=idx, columns=weights.index, dtype=float)\n",
    "    for d in ws.index:\n",
    "        ws.loc[d] = weights.values\n",
    "    return ws\n",
    "\n",
    "def simulate_rebalance(\n",
    "    prices_df: pd.DataFrame,\n",
    "    weights_schedule: pd.DataFrame,\n",
    "    rebalance_freq: str = 'M',\n",
    "    cost_per_side: float = COST_PER_SIDE,\n",
    "    slippage: float = 0.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simulate NAV with rebalancing at close, applying costs and tracking turnover\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Price data for assets\n",
    "    weights_schedule : pd.DataFrame\n",
    "        Rebalancing schedule (index: dates, columns: tickers)\n",
    "    rebalance_freq : str\n",
    "        Rebalancing frequency\n",
    "    cost_per_side : float\n",
    "        Transaction cost per side\n",
    "    slippage : float\n",
    "        Additional slippage cost\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with NAV series, turnover, costs, and KPIs\n",
    "    \"\"\"\n",
    "    px = prices_df.copy()\n",
    "    px = px[sorted(px.columns)]\n",
    "    rets = compute_returns(px)\n",
    "    ws = weights_schedule.copy().reindex(rets.index).dropna(how='all').fillna(0.0)\n",
    "    all_cols = sorted(set(px.columns) | set(ws.columns))\n",
    "    rets = rets.reindex(columns=all_cols).fillna(0.0)\n",
    "    ws = ws.reindex(columns=all_cols).fillna(0.0)\n",
    "\n",
    "    nav = 1.0\n",
    "    current_weights = np.zeros(len(all_cols), dtype=float)\n",
    "    nav_series = pd.Series(index=rets.index, dtype=float)\n",
    "    turnover_series = pd.Series(0.0, index=rets.index)\n",
    "    cum_cost_value = 0.0\n",
    "    cum_cost_events = 0\n",
    "\n",
    "    for t in rets.index:\n",
    "        r_t = rets.loc[t].values\n",
    "        day_ret = float(np.nansum(current_weights * r_t))\n",
    "        nav *= (1.0 + day_ret)\n",
    "\n",
    "        if t in ws.index and not ws.loc[t].isna().all():\n",
    "            target = ws.loc[t].values.astype(float)\n",
    "            w_eod_unnorm = current_weights * (1.0 + r_t)\n",
    "            s = w_eod_unnorm.sum()\n",
    "            w_eod = w_eod_unnorm / s if s > 0 else current_weights.copy()\n",
    "            delta = target - w_eod\n",
    "            turnover = float(np.sum(np.abs(delta)))\n",
    "            cost = (cost_per_side + slippage) * turnover * nav\n",
    "            nav -= cost\n",
    "            cum_cost_value += float(cost)\n",
    "            cum_cost_events += 1 if turnover > 0 else 0\n",
    "            turnover_series.loc[t] = turnover\n",
    "            current_weights = target.copy()\n",
    "\n",
    "        nav_series.loc[t] = nav\n",
    "\n",
    "    returns_net = nav_series.pct_change().fillna(0.0)\n",
    "    kpis_net = compute_kpis(returns_net)\n",
    "    return {\n",
    "        'nav': nav_series,\n",
    "        'turnover': turnover_series[turnover_series > 0],\n",
    "        'cum_cost': float(cum_cost_value),\n",
    "        'cum_cost_events': float(cum_cost_events),\n",
    "        'kpis_net': kpis_net,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "walk_forward",
   "metadata": {},
   "source": [
    "## WALK-FORWARD ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "walk_forward_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WALK-FORWARD ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "walk_forward_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_evaluate(\n",
    "    prices_df: pd.DataFrame,\n",
    "    train_window: int = TRAIN_WINDOW,\n",
    "    test_window: int = TEST_WINDOW,\n",
    "    rebalance_freq: str = REBALANCE_FREQUENCY,\n",
    "    params: dict = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Walk-forward validation pipeline with comprehensive analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Price data for all assets\n",
    "    train_window : int\n",
    "        Training window size in days\n",
    "    test_window : int\n",
    "        Test window size in days\n",
    "    rebalance_freq : str\n",
    "        Rebalancing frequency\n",
    "    params : dict\n",
    "        Parameters for optimization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with fold results, aggregated KPIs, and OOS NAV\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'N_stocks': N_STOCKS_AUTO,\n",
    "            'selection_method': 'exhaustive' if len(prices_df.columns) <= 10 else 'greedy',\n",
    "            'objective': 'max_sharpe',\n",
    "            'opt_method': 'scipy',\n",
    "            'turnover_penalty': 0.0,\n",
    "            'cost_per_side': COST_PER_SIDE,\n",
    "            'max_weight': MAX_WEIGHT,\n",
    "            'seed': RNG_SEED,\n",
    "        }\n",
    "    \n",
    "    px = prices_df.copy()\n",
    "    dates = px.index\n",
    "    folds = []\n",
    "    all_nav = []\n",
    "    i = train_window\n",
    "    prev_w = None\n",
    "    rng_seed = params.get('seed', RNG_SEED)\n",
    "    \n",
    "    logger.info(f\"Starting walk-forward analysis: train={train_window}, test={test_window}\")\n",
    "    \n",
    "    while i + test_window <= len(dates):\n",
    "        train_start = dates[i - train_window]\n",
    "        train_end = dates[i - 1]\n",
    "        test_start = dates[i]\n",
    "        test_end = dates[i + test_window - 1]\n",
    "        \n",
    "        logger.info(f\"Fold: Train {train_start.date()} to {train_end.date()}, Test {test_start.date()} to {test_end.date()}\")\n",
    "        \n",
    "        px_tr = px.loc[train_start:train_end]\n",
    "\n",
    "        # Stock selection\n",
    "        sel = select_optimal_stocks(\n",
    "            px_tr,\n",
    "            N_stocks=params.get('N_stocks', N_STOCKS_AUTO),\n",
    "            lookback_days=train_window,\n",
    "            method=params.get('selection_method', 'exhaustive' if px_tr.shape[1] <= 10 else 'greedy'),\n",
    "            seed=rng_seed,\n",
    "        )\n",
    "        tickers = sel['selected_tickers']\n",
    "        \n",
    "        # Weight optimization\n",
    "        rets_tr = compute_returns(px_tr[tickers]).dropna()\n",
    "        opt = optimize_weights(\n",
    "            rets_tr,\n",
    "            objective=params.get('objective', 'max_sharpe'),\n",
    "            bounds=(0.0, params.get('max_weight', MAX_WEIGHT)),\n",
    "            sum_to=1.0,\n",
    "            method=params.get('opt_method', 'scipy'),\n",
    "            turnover_penalty=params.get('turnover_penalty', 0.0),\n",
    "            prev_weights=prev_w if prev_w is not None else None,\n",
    "            seed=rng_seed,\n",
    "        )\n",
    "        w = opt['weights']\n",
    "        prev_w = w.reindex(tickers).fillna(0.0).values\n",
    "\n",
    "        # Out-of-sample simulation\n",
    "        ws = build_weights_schedule(test_start, test_end, w, freq=rebalance_freq)\n",
    "        sim = simulate_rebalance(\n",
    "            px[tickers].loc[dates[0]:test_end], \n",
    "            ws, \n",
    "            rebalance_freq=rebalance_freq, \n",
    "            cost_per_side=params.get('cost_per_side', COST_PER_SIDE)\n",
    "        )\n",
    "        nav = sim['nav'].loc[test_start:test_end]\n",
    "        all_nav.append(nav)\n",
    "        \n",
    "        # Calculate KPIs\n",
    "        kpis_is = opt['kpis']\n",
    "        kpis_oos = compute_kpis(nav.pct_change().fillna(0.0))\n",
    "        \n",
    "        folds.append({\n",
    "            'train_start': train_start, 'train_end': train_end,\n",
    "            'test_start': test_start, 'test_end': test_end,\n",
    "            'tickers': tickers, 'weights': w,\n",
    "            'kpis_is': kpis_is, 'kpis_oos': kpis_oos,\n",
    "        })\n",
    "        i += test_window\n",
    "\n",
    "    # Aggregate results\n",
    "    nav_oos = pd.concat(all_nav).sort_index() if all_nav else pd.Series(dtype=float)\n",
    "    agg_kpis = compute_kpis(nav_oos.pct_change().fillna(0.0)) if len(nav_oos) else {k: 0.0 for k in ('ann_return','ann_vol','sharpe','max_dd')}\n",
    "    \n",
    "    logger.info(f\"Walk-forward complete: {len(folds)} folds, OOS Sharpe: {agg_kpis.get('sharpe', 0):.3f}\")\n",
    "    \n",
    "    return {'folds': folds, 'agg_kpis_oos': agg_kpis, 'nav_oos': nav_oos}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reporting_functions",
   "metadata": {},
   "source": [
    "## REPORTING & SUMMARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reporting_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORTING & SUMMARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "reporting_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exec_summary(kpis_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate executive summary with key insights\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kpis_dict : dict\n",
    "        Dictionary with 'is', 'oos', and 'costs' keys\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Markdown formatted executive summary\n",
    "    \"\"\"\n",
    "    is_k = kpis_dict.get('is', {})\n",
    "    oos_k = kpis_dict.get('oos', {})\n",
    "    costs = kpis_dict.get('costs', {})\n",
    "    \n",
    "    lines = [\n",
    "        \"## 📊 Executive Summary\",\n",
    "        \"\",\n",
    "        f\"**Performance Overview:**\",\n",
    "        f\"- In-sample Sharpe Ratio: {is_k.get('sharpe', 0):.2f} | Out-of-sample Sharpe: {oos_k.get('sharpe', 0):.2f}\",\n",
    "        f\"- Out-of-sample Annual Return: {oos_k.get('ann_return', 0)*100:.2f}% | Volatility: {oos_k.get('ann_vol', 0)*100:.2f}%\",\n",
    "        f\"- Maximum Drawdown: {oos_k.get('max_dd', 0)*100:.2f}%\",\n",
    "        \"\",\n",
    "        f\"**Risk Management:**\",\n",
    "        f\"- Rebalancing events: {costs.get('events', 0):.0f}\",\n",
    "        f\"- Transaction cost per side: {costs.get('cost_per_side', COST_PER_SIDE)*10000:.1f} bps\",\n",
    "        \"\",\n",
    "        f\"**Recommendations:**\",\n",
    "        f\"- {'✅ Strong' if oos_k.get('sharpe', 0) > 1.0 else '⚠️ Moderate' if oos_k.get('sharpe', 0) > 0.5 else '❌ Weak'} risk-adjusted performance\",\n",
    "        f\"- {'✅ Acceptable' if abs(oos_k.get('max_dd', 0)) < 0.2 else '⚠️ High'} drawdown levels\",\n",
    "        f\"- Monitor tracking error and consider adjusting weight constraints if needed\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def load_benchmark_data(prices_df: pd.DataFrame, benchmark_ticker: str = BENCHMARK) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load benchmark data or create synthetic benchmark\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Universe price data\n",
    "    benchmark_ticker : str\n",
    "        Benchmark ticker symbol\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Benchmark price series\n",
    "    \"\"\"\n",
    "    # Try to load benchmark from data folder\n",
    "    benchmark_path = os.path.join(DATA_FOLDER, f\"{benchmark_ticker}.csv\")\n",
    "    \n",
    "    if os.path.exists(benchmark_path):\n",
    "        try:\n",
    "            benchmark_df = load_prices_from_csv([benchmark_ticker], START_DATE, END_DATE, DATA_FOLDER, strict=False)\n",
    "            if not benchmark_df.empty and benchmark_ticker in benchmark_df.columns:\n",
    "                benchmark_series = benchmark_df[benchmark_ticker].reindex(prices_df.index).fillna(method='ffill')\n",
    "                logger.info(f\"✅ Loaded {benchmark_ticker} benchmark data\")\n",
    "                return benchmark_series\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {benchmark_ticker}: {e}\")\n",
    "    \n",
    "    # Create synthetic equal-weight benchmark\n",
    "    logger.info(f\"Creating synthetic equal-weight benchmark from universe\")\n",
    "    returns = compute_returns(prices_df)\n",
    "    if returns.empty:\n",
    "        return pd.Series(index=prices_df.index, dtype=float)\n",
    "    \n",
    "    w = np.full(len(prices_df.columns), 1.0 / max(1, len(prices_df.columns)))\n",
    "    ew_ret = pd.Series(np.nansum(returns.values * w.reshape(1, -1), axis=1), index=returns.index)\n",
    "    ew_nav = (1 + ew_ret).cumprod()\n",
    "    return ew_nav / ew_nav.iloc[0] * 100  # Normalize to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_functions",
   "metadata": {},
   "source": [
    "## ENHANCED VISUALIZATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED VISUALIZATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "visualization_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_dashboard(portfolio_nav, benchmark_nav, portfolio_kpis, benchmark_kpis, weights_series):\n",
    "    \"\"\"\n",
    "    Create comprehensive performance dashboard with multiple charts\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolio_nav : pd.Series\n",
    "        Portfolio NAV series\n",
    "    benchmark_nav : pd.Series\n",
    "        Benchmark NAV series\n",
    "    portfolio_kpis : dict\n",
    "        Portfolio KPIs\n",
    "    benchmark_kpis : dict\n",
    "        Benchmark KPIs\n",
    "    weights_series : pd.Series\n",
    "        Final portfolio weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Interactive dashboard figure\n",
    "    \"\"\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Portfolio vs Benchmark Performance',\n",
    "            'Portfolio Allocation',\n",
    "            'Rolling Drawdown',\n",
    "            'Performance Metrics Comparison'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "               [{\"secondary_y\": False}, {\"type\": \"bar\"}]],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # 1. Performance comparison\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio_nav.index,\n",
    "            y=portfolio_nav.values,\n",
    "            name='Portfolio',\n",
    "            line=dict(color='#1f77b4', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=benchmark_nav.index,\n",
    "            y=benchmark_nav.values,\n",
    "            name='Benchmark',\n",
    "            line=dict(color='#ff7f0e', width=2, dash='dash')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Portfolio allocation pie chart\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=weights_series.index,\n",
    "            values=weights_series.values,\n",
    "            name=\"Portfolio Weights\",\n",
    "            textinfo='label+percent',\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Rolling drawdown\n",
    "    portfolio_dd = (portfolio_nav / portfolio_nav.cummax() - 1) * 100\n",
    "    benchmark_dd = (benchmark_nav / benchmark_nav.cummax() - 1) * 100\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio_dd.index,\n",
    "            y=portfolio_dd.values,\n",
    "            name='Portfolio DD',\n",
    "            fill='tonexty',\n",
    "            line=dict(color='red', width=1)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=benchmark_dd.index,\n",
    "            y=benchmark_dd.values,\n",
    "            name='Benchmark DD',\n",
    "            line=dict(color='orange', width=1, dash='dash')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Performance metrics comparison\n",
    "    metrics = ['ann_return', 'ann_vol', 'sharpe', 'max_dd']\n",
    "    portfolio_values = [portfolio_kpis.get(m, 0) * (100 if m in ['ann_return', 'ann_vol', 'max_dd'] else 1) for m in metrics]\n",
    "    benchmark_values = [benchmark_kpis.get(m, 0) * (100 if m in ['ann_return', 'ann_vol', 'max_dd'] else 1) for m in metrics]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Annual Return (%)', 'Volatility (%)', 'Sharpe Ratio', 'Max DD (%)'],\n",
    "            y=portfolio_values,\n",
    "            name='Portfolio',\n",
    "            marker_color='#1f77b4'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Annual Return (%)', 'Volatility (%)', 'Sharpe Ratio', 'Max DD (%)'],\n",
    "            y=benchmark_values,\n",
    "            name='Benchmark',\n",
    "            marker_color='#ff7f0e'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': '📊 Portfolio Optimization Dashboard',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'font': {'size': 20}\n",
    "        },\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"NAV\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_risk_metrics_chart(comprehensive_kpis):\n",
    "    \"\"\"\n",
    "    Create advanced risk metrics visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    comprehensive_kpis : dict\n",
    "        Dictionary with comprehensive KPIs including VaR, CVaR, etc.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Risk metrics chart\n",
    "    \"\"\"\n",
    "    # Risk metrics data\n",
    "    risk_metrics = {\n",
    "        'Sharpe Ratio': comprehensive_kpis.get('sharpe', 0),\n",
    "        'Sortino Ratio': comprehensive_kpis.get('sortino', 0),\n",
    "        'Information Ratio': comprehensive_kpis.get('info_ratio', 0),\n",
    "        'VaR (95%)': abs(comprehensive_kpis.get('var_95', 0)) * 100,\n",
    "        'CVaR (95%)': abs(comprehensive_kpis.get('cvar_95', 0)) * 100,\n",
    "        'Max Drawdown': abs(comprehensive_kpis.get('max_dd', 0)) * 100\n",
    "    }\n",
    "    \n",
    "    # Create radar chart for risk metrics\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Normalize values for radar chart (except ratios)\n",
    "    normalized_values = []\n",
    "    for metric, value in risk_metrics.items():\n",
    "        if 'Ratio' in metric:\n",
    "            normalized_values.append(max(0, min(value, 3)))  # Cap ratios at 3\n",
    "        else:\n",
    "            normalized_values.append(value)\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=normalized_values,\n",
    "        theta=list(risk_metrics.keys()),\n",
    "        fill='toself',\n",
    "        name='Risk Metrics',\n",
    "        line_color='rgb(1,90,120)',\n",
    "        fillcolor='rgba(1,90,120,0.2)'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, max(normalized_values) * 1.1]\n",
    "            )\n",
    "        ),\n",
    "        title={\n",
    "            'text': '🎯 Advanced Risk Metrics Profile',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'font': {'size': 16}\n",
    "        },\n",
    "        template='plotly_white',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_walk_forward_analysis_chart(walk_forward_results):\n",
    "    \"\"\"\n",
    "    Create walk-forward analysis visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    walk_forward_results : dict\n",
    "        Results from walk_forward_evaluate function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Walk-forward analysis chart\n",
    "    \"\"\"\n",
    "    folds = walk_forward_results['folds']\n",
    "    nav_oos = walk_forward_results['nav_oos']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(\n",
    "            'Out-of-Sample NAV Evolution',\n",
    "            'In-Sample vs Out-of-Sample Sharpe Ratios by Fold'\n",
    "        ),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # 1. OOS NAV evolution\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=nav_oos.index,\n",
    "            y=nav_oos.values,\n",
    "            name='OOS NAV',\n",
    "            line=dict(color='#2E8B57', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. IS vs OOS Sharpe ratios\n",
    "    fold_numbers = list(range(1, len(folds) + 1))\n",
    "    is_sharpe = [fold['kpis_is']['sharpe'] for fold in folds]\n",
    "    oos_sharpe = [fold['kpis_oos']['sharpe'] for fold in folds]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=fold_numbers,\n",
    "            y=is_sharpe,\n",
    "            name='In-Sample Sharpe',\n",
    "            marker_color='#4CAF50',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=fold_numbers,\n",
    "            y=oos_sharpe,\n",
    "            name='Out-of-Sample Sharpe',\n",
    "            marker_color='#FF9800',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': '🔄 Walk-Forward Analysis Results',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'font': {'size': 18}\n",
    "        },\n",
    "        height=700,\n",
    "        showlegend=True,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"NAV\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Fold Number\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Sharpe Ratio\", row=2, col=1)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_execution",
   "metadata": {},
   "source": [
    "## MAIN EXECUTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_execution_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN EXECUTION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_data_loading",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 17:06:32,574 - INFO - Loading price data for 10 tickers...\n",
      "2025-08-22 17:06:32,575 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/AAPL.csv\n",
      "2025-08-22 17:06:32,576 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/MSFT.csv\n",
      "2025-08-22 17:06:32,577 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/GOOGL.csv\n",
      "2025-08-22 17:06:32,577 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/AMZN.csv\n",
      "2025-08-22 17:06:32,578 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/NVDA.csv\n",
      "2025-08-22 17:06:32,578 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/META.csv\n",
      "2025-08-22 17:06:32,579 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/TSLA.csv\n",
      "2025-08-22 17:06:32,579 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/NFLX.csv\n",
      "2025-08-22 17:06:32,580 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/AMD.csv\n",
      "2025-08-22 17:06:32,580 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/CRM.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading stock price data...\n",
      "❌ Error loading data: No valid stock data loaded!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid stock data loaded!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Loading stock price data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Load stock prices\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     stock_prices \u001b[38;5;241m=\u001b[39m load_prices_from_csv(\n\u001b[1;32m      7\u001b[0m         tickers\u001b[38;5;241m=\u001b[39mSTOCK_UNIVERSE,\n\u001b[1;32m      8\u001b[0m         start_date\u001b[38;5;241m=\u001b[39mSTART_DATE,\n\u001b[1;32m      9\u001b[0m         end_date\u001b[38;5;241m=\u001b[39mEND_DATE,\n\u001b[1;32m     10\u001b[0m         data_folder\u001b[38;5;241m=\u001b[39mDATA_FOLDER,\n\u001b[1;32m     11\u001b[0m         strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Load benchmark data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     benchmark_prices \u001b[38;5;241m=\u001b[39m load_benchmark_data(stock_prices, BENCHMARK)\n",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m, in \u001b[0;36mload_prices_from_csv\u001b[0;34m(tickers, start_date, end_date, data_folder, strict)\u001b[0m\n\u001b[1;32m     56\u001b[0m         failed_tickers\u001b[38;5;241m.\u001b[39mappend(ticker)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stock_data:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid stock data loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Create aligned DataFrame\u001b[39;00m\n\u001b[1;32m     62\u001b[0m stock_adj_close \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stock_data)\n",
      "\u001b[0;31mValueError\u001b[0m: No valid stock data loaded!"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and validate stock price data\n",
    "print(\"🔄 Cloning repo and locating data...\")\n",
    "\n",
    "# Clone repo if needed\n",
    "REPO_URL = \"https://github.com/maxiveloso/python-portfolio-optimization.git\"\n",
    "REPO_DIR = \"/content/python-portfolio-optimization\"\n",
    "DATA_FOLDER = os.path.join(REPO_DIR, \"data\")\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"📥 Cloning GitHub repo...\")\n",
    "    !git clone -q {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Confirm data files are present\n",
    "files = os.listdir(DATA_FOLDER)\n",
    "tickers_found = [f.replace(\".csv\", \"\") for f in files if f.endswith(\".csv\")]\n",
    "print(f\"📂 Found {len(tickers_found)} tickers in repo data folder: {', '.join(tickers_found[:10])}...\")\n",
    "\n",
    "# Load data using your existing function\n",
    "try:\n",
    "    stock_prices = load_prices_from_csv(\n",
    "        tickers=STOCK_UNIVERSE,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        data_folder=DATA_FOLDER,\n",
    "        strict=False\n",
    "    )\n",
    "    \n",
    "    benchmark_prices = load_benchmark_data(stock_prices, BENCHMARK)\n",
    "    \n",
    "    print(f\"✅ Data loaded successfully:\")\n",
    "    print(f\"   📈 Stocks: {len(stock_prices.columns)} assets, {len(stock_prices)} days\")\n",
    "    print(f\"   🎯 Benchmark: {len(benchmark_prices)} days\")\n",
    "    print(f\"   📅 Date range: {stock_prices.index[0].date()} to {stock_prices.index[-1].date()}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    returns = compute_returns(stock_prices)\n",
    "    print(f\"\\n📊 Universe Statistics:\")\n",
    "    print(f\"   Average daily return: {returns.mean().mean()*100:.3f}%\")\n",
    "    print(f\"   Average daily volatility: {returns.std().mean()*100:.3f}%\")\n",
    "    print(f\"   Average correlation: {average_offdiag_correlation(returns):.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_stock_selection",
   "metadata": {},
   "source": [
    "### Step 2: Stock Selection and Portfolio Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stock_selection_optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:02:00,670 - INFO - Using exhaustive search for 10 assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Performing stock selection and optimization...\n",
      "✅ Selected 4 stocks: ['NVDA', 'META', 'NFLX', 'CRM']\n",
      "\n",
      "🎯 Optimizing for max_sharpe...\n",
      "   📊 max_sharpe: Sharpe=1.182, Return=48.48%, Vol=39.31%\n",
      "\n",
      "🎯 Optimizing for min_var...\n",
      "   📊 min_var: Sharpe=0.759, Return=28.63%, Vol=35.11%\n",
      "\n",
      "🎯 Optimizing for risk_parity...\n",
      "   📊 risk_parity: Sharpe=1.041, Return=40.10%, Vol=36.61%\n",
      "\n",
      "🏆 Primary Strategy (Max Sharpe):\n",
      "   Sharpe Ratio: 1.182\n",
      "   Annual Return: 48.48%\n",
      "   Annual Volatility: 39.31%\n",
      "\n",
      "💼 Portfolio Weights:\n",
      "   NVDA: 40.00%\n",
      "   META: 24.11%\n",
      "   NFLX: 27.62%\n",
      "   CRM: 8.28%\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Stock selection and portfolio optimization\n",
    "print(\"🔄 Performing stock selection and optimization...\")\n",
    "\n",
    "# Stock selection\n",
    "lookback_days = min(504, len(stock_prices))  # 2 years or available data\n",
    "selection_method = 'exhaustive' if len(stock_prices.columns) <= 10 else 'greedy'\n",
    "\n",
    "selection_results = select_optimal_stocks(\n",
    "    prices_df=stock_prices,\n",
    "    N_stocks=N_STOCKS_AUTO,\n",
    "    lookback_days=lookback_days,\n",
    "    method=selection_method,\n",
    "    seed=RNG_SEED\n",
    ")\n",
    "\n",
    "selected_tickers = selection_results['selected_tickers']\n",
    "print(f\"✅ Selected {len(selected_tickers)} stocks: {selected_tickers}\")\n",
    "\n",
    "# Portfolio optimization with multiple objectives\n",
    "selected_prices = stock_prices[selected_tickers]\n",
    "selected_returns = compute_returns(selected_prices)\n",
    "\n",
    "# Optimize for different objectives\n",
    "objectives = ['max_sharpe', 'min_var', 'risk_parity']\n",
    "optimization_results = {}\n",
    "\n",
    "for objective in objectives:\n",
    "    print(f\"\\n🎯 Optimizing for {objective}...\")\n",
    "    opt_result = optimize_weights(\n",
    "        returns_df=selected_returns,\n",
    "        objective=objective,\n",
    "        bounds=(0.0, MAX_WEIGHT),\n",
    "        sum_to=1.0,\n",
    "        method='scipy',\n",
    "        risk_free=RISK_FREE_RATE,\n",
    "        seed=RNG_SEED\n",
    "    )\n",
    "    optimization_results[objective] = opt_result\n",
    "    \n",
    "    kpis = opt_result['kpis']\n",
    "    print(f\"   📊 {objective}: Sharpe={kpis['sharpe']:.3f}, Return={kpis['ann_return']*100:.2f}%, Vol={kpis['ann_vol']*100:.2f}%\")\n",
    "\n",
    "# Use max_sharpe as primary strategy\n",
    "primary_weights = optimization_results['max_sharpe']['weights']\n",
    "primary_kpis = optimization_results['max_sharpe']['kpis']\n",
    "\n",
    "print(f\"\\n🏆 Primary Strategy (Max Sharpe):\")\n",
    "print(f\"   Sharpe Ratio: {primary_kpis['sharpe']:.3f}\")\n",
    "print(f\"   Annual Return: {primary_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   Annual Volatility: {primary_kpis['ann_vol']*100:.2f}%\")\n",
    "print(f\"\\n💼 Portfolio Weights:\")\n",
    "for ticker, weight in primary_weights.items():\n",
    "    print(f\"   {ticker}: {weight*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_comprehensive_analysis",
   "metadata": {},
   "source": [
    "### Step 3: Comprehensive KPI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comprehensive_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Calculating comprehensive KPIs...\n",
      "\n",
      "📊 Comprehensive Portfolio Analysis:\n",
      "   📈 Annual Return: 48.48%\n",
      "   📉 Annual Volatility: 39.30%\n",
      "   ⚡ Sharpe Ratio: 1.183\n",
      "   🎯 Sortino Ratio: 1.648\n",
      "   📊 Information Ratio: 1.363\n",
      "   🔻 Maximum Drawdown: -62.58%\n",
      "   ⚠️ VaR (95%): -60.50%\n",
      "   🚨 CVaR (95%): -88.51%\n",
      "   🔢 Alpha: 21.29%\n",
      "   📐 Beta: 1.357\n",
      "\n",
      "🎯 vs QQQ Benchmark:\n",
      "   📊 Correlation: 0.884\n",
      "   📈 Active Return: 27.91%\n",
      "   📉 Tracking Error: 20.48%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate comprehensive KPIs\n",
    "print(\"🔄 Calculating comprehensive KPIs...\")\n",
    "\n",
    "# Calculate portfolio returns\n",
    "portfolio_returns = (selected_returns * primary_weights).sum(axis=1)\n",
    "benchmark_returns = compute_returns(benchmark_prices.to_frame('benchmark'))['benchmark']\n",
    "\n",
    "# Calculate comprehensive KPIs\n",
    "comprehensive_kpis = calculate_comprehensive_kpis(\n",
    "    returns=portfolio_returns,\n",
    "    benchmark_returns=benchmark_returns,\n",
    "    rf=RISK_FREE_RATE\n",
    ")\n",
    "\n",
    "benchmark_kpis = calculate_comprehensive_kpis(\n",
    "    returns=benchmark_returns,\n",
    "    rf=RISK_FREE_RATE\n",
    ")\n",
    "\n",
    "# Display comprehensive metrics\n",
    "print(f\"\\n📊 Comprehensive Portfolio Analysis:\")\n",
    "print(f\"   📈 Annual Return: {comprehensive_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   📉 Annual Volatility: {comprehensive_kpis['ann_vol']*100:.2f}%\")\n",
    "print(f\"   ⚡ Sharpe Ratio: {comprehensive_kpis['sharpe']:.3f}\")\n",
    "print(f\"   🎯 Sortino Ratio: {comprehensive_kpis['sortino']:.3f}\")\n",
    "print(f\"   📊 Information Ratio: {comprehensive_kpis['info_ratio']:.3f}\")\n",
    "print(f\"   🔻 Maximum Drawdown: {comprehensive_kpis['max_dd']*100:.2f}%\")\n",
    "print(f\"   ⚠️ VaR (95%): {comprehensive_kpis['var_95']*100:.2f}%\")\n",
    "print(f\"   🚨 CVaR (95%): {comprehensive_kpis['cvar_95']*100:.2f}%\")\n",
    "print(f\"   🔢 Alpha: {comprehensive_kpis['alpha']*100:.2f}%\")\n",
    "print(f\"   📐 Beta: {comprehensive_kpis['beta']:.3f}\")\n",
    "\n",
    "# Benchmark comparison\n",
    "vs_benchmark = kpis_vs_benchmark(portfolio_returns, benchmark_returns)\n",
    "print(f\"\\n🎯 vs {BENCHMARK} Benchmark:\")\n",
    "print(f\"   📊 Correlation: {vs_benchmark['corr']:.3f}\")\n",
    "print(f\"   📈 Active Return: {vs_benchmark['active_return']*100:.2f}%\")\n",
    "print(f\"   📉 Tracking Error: {vs_benchmark['tracking_error']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_walk_forward",
   "metadata": {},
   "source": [
    "### Step 4: Walk-Forward Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "walk_forward_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:02:37,951 - INFO - Starting walk-forward analysis: train=252, test=63\n",
      "2025-08-21 19:02:37,952 - INFO - Fold: Train 2020-01-02 to 2020-12-30, Test 2020-12-31 to 2021-04-01\n",
      "2025-08-21 19:02:37,957 - INFO - Using exhaustive search for 10 assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Performing walk-forward analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:02:38,358 - INFO - Fold: Train 2020-04-02 to 2021-04-01, Test 2021-04-05 to 2021-07-01\n",
      "2025-08-21 19:02:38,360 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:38,782 - INFO - Fold: Train 2020-07-02 to 2021-07-01, Test 2021-07-02 to 2021-09-30\n",
      "2025-08-21 19:02:38,784 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:39,188 - INFO - Fold: Train 2020-10-01 to 2021-09-30, Test 2021-10-01 to 2021-12-30\n",
      "2025-08-21 19:02:39,190 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:39,581 - INFO - Fold: Train 2020-12-31 to 2021-12-30, Test 2021-12-31 to 2022-03-31\n",
      "2025-08-21 19:02:39,583 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:39,990 - INFO - Fold: Train 2021-04-05 to 2022-03-31, Test 2022-04-01 to 2022-07-01\n",
      "2025-08-21 19:02:39,991 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:40,357 - INFO - Fold: Train 2021-07-02 to 2022-07-01, Test 2022-07-05 to 2022-09-30\n",
      "2025-08-21 19:02:40,358 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:40,697 - INFO - Fold: Train 2021-10-01 to 2022-09-30, Test 2022-10-03 to 2022-12-30\n",
      "2025-08-21 19:02:40,699 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:41,057 - INFO - Fold: Train 2021-12-31 to 2022-12-30, Test 2023-01-03 to 2023-04-03\n",
      "2025-08-21 19:02:41,059 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:41,441 - INFO - Fold: Train 2022-04-01 to 2023-04-03, Test 2023-04-04 to 2023-07-05\n",
      "2025-08-21 19:02:41,443 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:42,090 - INFO - Fold: Train 2022-07-05 to 2023-07-05, Test 2023-07-06 to 2023-10-03\n",
      "2025-08-21 19:02:42,092 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:42,491 - INFO - Fold: Train 2022-10-03 to 2023-10-03, Test 2023-10-04 to 2024-01-03\n",
      "2025-08-21 19:02:42,493 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:42,924 - INFO - Fold: Train 2023-01-03 to 2024-01-03, Test 2024-01-04 to 2024-04-04\n",
      "2025-08-21 19:02:42,926 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:43,431 - INFO - Fold: Train 2023-04-04 to 2024-04-04, Test 2024-04-05 to 2024-07-05\n",
      "2025-08-21 19:02:43,433 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:43,861 - INFO - Fold: Train 2023-07-06 to 2024-07-05, Test 2024-07-08 to 2024-10-03\n",
      "2025-08-21 19:02:43,862 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:44,323 - INFO - Walk-forward complete: 15 folds, OOS Sharpe: 0.238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Walk-Forward Analysis Results (15 folds):\n",
      "   📈 OOS Annual Return: 9.20%\n",
      "   📉 OOS Annual Volatility: 38.59%\n",
      "   ⚡ OOS Sharpe Ratio: 0.238\n",
      "   🔻 OOS Maximum Drawdown: -36.79%\n",
      "\n",
      "📊 Average Performance Across Folds:\n",
      "   🎯 Average IS Sharpe: 1.887\n",
      "   🎯 Average OOS Sharpe: 0.488\n",
      "   📉 Performance Decay: 74.1%\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Walk-forward validation\n",
    "print(\"🔄 Performing walk-forward analysis...\")\n",
    "\n",
    "# Walk-forward parameters\n",
    "wf_params = {\n",
    "    'N_stocks': N_STOCKS_AUTO,\n",
    "    'selection_method': selection_method,\n",
    "    'objective': 'max_sharpe',\n",
    "    'opt_method': 'scipy',\n",
    "    'turnover_penalty': 0.0,\n",
    "    'cost_per_side': COST_PER_SIDE,\n",
    "    'max_weight': MAX_WEIGHT,\n",
    "    'seed': RNG_SEED,\n",
    "}\n",
    "\n",
    "# Perform walk-forward analysis\n",
    "walk_forward_results = walk_forward_evaluate(\n",
    "    prices_df=stock_prices,\n",
    "    train_window=TRAIN_WINDOW,\n",
    "    test_window=TEST_WINDOW,\n",
    "    rebalance_freq=REBALANCE_FREQUENCY,\n",
    "    params=wf_params\n",
    ")\n",
    "\n",
    "# Display walk-forward results\n",
    "oos_kpis = walk_forward_results['agg_kpis_oos']\n",
    "n_folds = len(walk_forward_results['folds'])\n",
    "\n",
    "print(f\"\\n🔄 Walk-Forward Analysis Results ({n_folds} folds):\")\n",
    "print(f\"   📈 OOS Annual Return: {oos_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   📉 OOS Annual Volatility: {oos_kpis['ann_vol']*100:.2f}%\")\n",
    "print(f\"   ⚡ OOS Sharpe Ratio: {oos_kpis['sharpe']:.3f}\")\n",
    "print(f\"   🔻 OOS Maximum Drawdown: {oos_kpis['max_dd']*100:.2f}%\")\n",
    "\n",
    "# Calculate average IS vs OOS performance\n",
    "avg_is_sharpe = np.mean([fold['kpis_is']['sharpe'] for fold in walk_forward_results['folds']])\n",
    "avg_oos_sharpe = np.mean([fold['kpis_oos']['sharpe'] for fold in walk_forward_results['folds']])\n",
    "\n",
    "print(f\"\\n📊 Average Performance Across Folds:\")\n",
    "print(f\"   🎯 Average IS Sharpe: {avg_is_sharpe:.3f}\")\n",
    "print(f\"   🎯 Average OOS Sharpe: {avg_oos_sharpe:.3f}\")\n",
    "print(f\"   📉 Performance Decay: {((avg_is_sharpe - avg_oos_sharpe) / avg_is_sharpe * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_simulation",
   "metadata": {},
   "source": [
    "### Step 5: Portfolio Simulation with Rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "portfolio_simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Simulating portfolio with rebalancing...\n",
      "\n",
      "💰 Portfolio Simulation Results:\n",
      "   🏁 Final Portfolio Value: $75,498.35\n",
      "   📈 Total Return: 654.98%\n",
      "   🔄 Rebalancing Events: 42\n",
      "   💸 Total Transaction Costs: $23.85\n",
      "   📊 Average Turnover per Event: 3.48%\n",
      "\n",
      "🎯 vs QQQ Benchmark:\n",
      "   📈 Portfolio Return: 654.98%\n",
      "   📊 Benchmark Return: 136.51%\n",
      "   🏆 Excess Return: 518.48%\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Portfolio simulation with rebalancing\n",
    "print(\"🔄 Simulating portfolio with rebalancing...\")\n",
    "\n",
    "# Create rebalancing schedule\n",
    "simulation_start = stock_prices.index[0]\n",
    "simulation_end = stock_prices.index[-1]\n",
    "\n",
    "weights_schedule = build_weights_schedule(\n",
    "    start_date=simulation_start,\n",
    "    end_date=simulation_end,\n",
    "    weights=primary_weights,\n",
    "    freq=REBALANCE_FREQUENCY\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "simulation_results = simulate_rebalance(\n",
    "    prices_df=selected_prices,\n",
    "    weights_schedule=weights_schedule,\n",
    "    rebalance_freq=REBALANCE_FREQUENCY,\n",
    "    cost_per_side=COST_PER_SIDE,\n",
    "    slippage=0.0\n",
    ")\n",
    "\n",
    "portfolio_nav = simulation_results['nav']\n",
    "turnover_events = simulation_results['turnover']\n",
    "total_costs = simulation_results['cum_cost']\n",
    "cost_events = simulation_results['cum_cost_events']\n",
    "\n",
    "# Calculate final portfolio value\n",
    "final_value = portfolio_nav.iloc[-1] * PORTFOLIO_VALUE\n",
    "total_return = (portfolio_nav.iloc[-1] - 1) * 100\n",
    "\n",
    "print(f\"\\n💰 Portfolio Simulation Results:\")\n",
    "print(f\"   🏁 Final Portfolio Value: ${final_value:,.2f}\")\n",
    "print(f\"   📈 Total Return: {total_return:.2f}%\")\n",
    "print(f\"   🔄 Rebalancing Events: {cost_events:.0f}\")\n",
    "print(f\"   💸 Total Transaction Costs: ${total_costs * PORTFOLIO_VALUE:.2f}\")\n",
    "print(f\"   📊 Average Turnover per Event: {turnover_events.mean()*100:.2f}%\")\n",
    "\n",
    "# Benchmark comparison\n",
    "benchmark_nav = (1 + benchmark_returns).cumprod()\n",
    "benchmark_final_value = benchmark_nav.iloc[-1] * PORTFOLIO_VALUE\n",
    "benchmark_total_return = (benchmark_nav.iloc[-1] - 1) * 100\n",
    "\n",
    "print(f\"\\n🎯 vs {BENCHMARK} Benchmark:\")\n",
    "print(f\"   📈 Portfolio Return: {total_return:.2f}%\")\n",
    "print(f\"   📊 Benchmark Return: {benchmark_total_return:.2f}%\")\n",
    "print(f\"   🏆 Excess Return: {(total_return - benchmark_total_return):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_executive_summary",
   "metadata": {},
   "source": [
    "### Step 6: Executive Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "executive_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating executive summary...\n",
      "\n",
      "============================================================\n",
      "## 📊 Executive Summary\n",
      "\n",
      "**Performance Overview:**\n",
      "- In-sample Sharpe Ratio: 1.18 | Out-of-sample Sharpe: 0.24\n",
      "- Out-of-sample Annual Return: 9.20% | Volatility: 38.59%\n",
      "- Maximum Drawdown: -36.79%\n",
      "\n",
      "**Risk Management:**\n",
      "- Rebalancing events: 42\n",
      "- Transaction cost per side: 10.0 bps\n",
      "\n",
      "**Recommendations:**\n",
      "- ❌ Weak risk-adjusted performance\n",
      "- ⚠️ High drawdown levels\n",
      "- Monitor tracking error and consider adjusting weight constraints if needed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Generate executive summary\n",
    "print(\"🔄 Generating executive summary...\")\n",
    "\n",
    "# Prepare summary data\n",
    "summary_data = {\n",
    "    'is': comprehensive_kpis,\n",
    "    'oos': oos_kpis,\n",
    "    'costs': {\n",
    "        'events': cost_events,\n",
    "        'cost_per_side': COST_PER_SIDE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate executive summary\n",
    "exec_summary = generate_exec_summary(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(exec_summary)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_visualizations",
   "metadata": {},
   "source": [
    "### Step 7: Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "enhanced_visualizations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating enhanced visualizations...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'portfolio_nav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Creating enhanced visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Performance Dashboard\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dashboard_fig \u001b[38;5;241m=\u001b[39m create_performance_dashboard(\n\u001b[0;32m----> 6\u001b[0m     portfolio_nav\u001b[38;5;241m=\u001b[39mportfolio_nav,\n\u001b[1;32m      7\u001b[0m     benchmark_nav\u001b[38;5;241m=\u001b[39mbenchmark_nav,\n\u001b[1;32m      8\u001b[0m     portfolio_kpis\u001b[38;5;241m=\u001b[39mcomprehensive_kpis,\n\u001b[1;32m      9\u001b[0m     benchmark_kpis\u001b[38;5;241m=\u001b[39mbenchmark_kpis,\n\u001b[1;32m     10\u001b[0m     weights_series\u001b[38;5;241m=\u001b[39mprimary_weights\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m dashboard_fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'portfolio_nav' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 7: Create enhanced visualizations\n",
    "print(\"🔄 Creating enhanced visualizations...\")\n",
    "\n",
    "# 1. Performance Dashboard\n",
    "dashboard_fig = create_performance_dashboard(\n",
    "    portfolio_nav=portfolio_nav,\n",
    "    benchmark_nav=benchmark_nav,\n",
    "    portfolio_kpis=comprehensive_kpis,\n",
    "    benchmark_kpis=benchmark_kpis,\n",
    "    weights_series=primary_weights\n",
    ")\n",
    "\n",
    "dashboard_fig.show()\n",
    "try:\n",
    "    dashboard_fig.write_html('/home/ubuntu/portfolio_dashboard.html')\n",
    "except Exception as e:\n",
    "    print(\"Could not write dashboard to /home/ubuntu/portfolio_dashboard.html — file not saved. (\", e, \")\")\n",
    "\n",
    "# 2. Risk Metrics Chart\n",
    "risk_fig = create_risk_metrics_chart(comprehensive_kpis)\n",
    "risk_fig.show()\n",
    "try:\n",
    "    risk_fig.write_html('/home/ubuntu/risk_metrics.html')\n",
    "except Exception as e:\n",
    "    print(\"Could not write dashboard to /home/ubuntu/risk_metrics.html — file not saved. (\", e, \")\")\n",
    "\n",
    "# 3. Walk-Forward Analysis Chart\n",
    "wf_fig = create_walk_forward_analysis_chart(walk_forward_results)\n",
    "wf_fig.show()\n",
    "try:\n",
    "    wf_fig.write_html('/home/ubuntu/walk_forward_analysis.html')\n",
    "except Exception as e:\n",
    "    print(\"Could not write dashboard to /home/ubuntu/walk_forward_analysis.html — file not saved. (\", e, \")\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ Visualizations created and saved:\")\n",
    "print(\"   📊 portfolio_dashboard.html\")\n",
    "print(\"   🎯 risk_metrics.html\")\n",
    "print(\"   🔄 walk_forward_analysis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8_export_results",
   "metadata": {},
   "source": [
    "### Step 8: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Export results to CSV files\n",
    "print(\"🔄 Exporting results...\")\n",
    "\n",
    "# Export portfolio weights\n",
    "primary_weights.to_csv('/home/ubuntu/final_portfolio_weights.csv', header=['Weight'])\n",
    "\n",
    "# Export NAV series\n",
    "nav_export = pd.DataFrame({\n",
    "    'Date': portfolio_nav.index,\n",
    "    'Portfolio_NAV': portfolio_nav.values,\n",
    "    'Benchmark_NAV': benchmark_nav.reindex(portfolio_nav.index).fillna(method='ffill').values\n",
    "})\n",
    "nav_export.to_csv('/home/ubuntu/nav_series.csv', index=False)\n",
    "\n",
    "# Export comprehensive KPIs\n",
    "kpis_export = pd.DataFrame({\n",
    "    'Metric': list(comprehensive_kpis.keys()),\n",
    "    'Portfolio': list(comprehensive_kpis.values()),\n",
    "    'Benchmark': [benchmark_kpis.get(k, np.nan) for k in comprehensive_kpis.keys()]\n",
    "})\n",
    "kpis_export.to_csv('/home/ubuntu/comprehensive_kpis.csv', index=False)\n",
    "\n",
    "# Export walk-forward results\n",
    "wf_export = []\n",
    "for i, fold in enumerate(walk_forward_results['folds']):\n",
    "    wf_export.append({\n",
    "        'Fold': i + 1,\n",
    "        'Train_Start': fold['train_start'],\n",
    "        'Train_End': fold['train_end'],\n",
    "        'Test_Start': fold['test_start'],\n",
    "        'Test_End': fold['test_end'],\n",
    "        'IS_Sharpe': fold['kpis_is']['sharpe'],\n",
    "        'IS_Return': fold['kpis_is']['ann_return'],\n",
    "        'IS_Vol': fold['kpis_is']['ann_vol'],\n",
    "        'OOS_Sharpe': fold['kpis_oos']['sharpe'],\n",
    "        'OOS_Return': fold['kpis_oos']['ann_return'],\n",
    "        'OOS_Vol': fold['kpis_oos']['ann_vol'],\n",
    "        'Selected_Tickers': '|'.join(fold['tickers'])\n",
    "    })\n",
    "\n",
    "wf_df = pd.DataFrame(wf_export)\n",
    "wf_df.to_csv('/home/ubuntu/walk_forward_results.csv', index=False)\n",
    "\n",
    "# Export executive summary\n",
    "with open('/home/ubuntu/executive_summary.md', 'w') as f:\n",
    "    f.write(exec_summary)\n",
    "\n",
    "print(\"\\n✅ Results exported successfully:\")\n",
    "print(\"   💼 final_portfolio_weights.csv\")\n",
    "print(\"   📈 nav_series.csv\")\n",
    "print(\"   📊 comprehensive_kpis.csv\")\n",
    "print(\"   🔄 walk_forward_results.csv\")\n",
    "print(\"   📝 executive_summary.md\")\n",
    "\n",
    "print(\"\\n🎉 Portfolio optimization analysis complete!\")\n",
    "print(f\"\\n🏆 Final Results Summary:\")\n",
    "print(f\"   Selected Assets: {', '.join(selected_tickers)}\")\n",
    "print(f\"   Portfolio Sharpe Ratio: {comprehensive_kpis['sharpe']:.3f}\")\n",
    "print(f\"   Annual Return: {comprehensive_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   Maximum Drawdown: {comprehensive_kpis['max_dd']*100:.2f}%\")\n",
    "print(f\"   Walk-Forward OOS Sharpe: {oos_kpis['sharpe']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
