{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd13b02",
   "metadata": {},
   "source": [
    "# Portfolio Optimization - Case Study\n",
    "\n",
    "## Business Overview\n",
    "**Client**: Investment Management Firm  \n",
    "**Objective**: Build optimized portfolios with superior risk-adjusted returns vs benchmark  \n",
    "**Investment Horizon**: Medium to long-term (2+ years)\n",
    "\n",
    "**Comparison Framework**: Equal-Weight vs sharpe-Optimized vs Benchmark (QQQ)\n",
    "\n",
    "**Key Constraints**: \n",
    "- No short selling allowed\n",
    "- No leverage permitted  \n",
    "- Monthly rebalancing frequency\n",
    "\n",
    "## Expected Deliverables\n",
    "- Executive KPI dashboard with key metrics (CAGR, sharpe, MDD, Calmar)\n",
    "- Risk-return visualization with efficient frontier\n",
    "- Stress test analysis across market regimes\n",
    "- Reproducible results with documented assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711414-a2d3-4470-89fc-fadc5964f6cd",
   "metadata": {},
   "source": [
    "# Configuration, Global Parameters, Logging & Reproducibility Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f594a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION & SETUP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "from itertools import combinations\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Configure display and warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.options.display.float_format = '{:,.6f}'.format # Consistent numeric formatting\n",
    "warnings.filterwarnings('ignore') # Suppress non-critical warnings for cleaner\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0ffa8-dba4-439f-8540-b834ed42fc4f",
   "metadata": {},
   "source": [
    "This block sets up the analysis environment and global configuration for the portfolio optimization case study. It:\n",
    "\n",
    "- Imports all required libraries for data handling, plotting, optimization, and statistics.\n",
    "Configures display options, warning suppression, and plotting style, and ensures plots render inline in Jupyter.\n",
    "- Resolves the data folder path robustly for both script and notebook contexts.\n",
    "- Defines all key global parameters that govern the backtest: asset universe, date range, operating mode, risk constraints, benchmark, transaction costs, Monte Carlo settings, and walk-forward evaluation windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b42433-1755-4d29-a7d2-6e50a34f11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PARAMETERS - EDIT THESE FOR DIFFERENT SCENARIOS\n",
    "\n",
    "# Get the directory of the current notebook and navigate to the data folder\n",
    "try:\n",
    "    # Use __file__ when the notebook is run as a script (e.g., in a Binder or Colab environment)\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    DATA_FOLDER = os.path.join(NOTEBOOK_DIR, '..', 'data')\n",
    "except NameError:\n",
    "    # Fallback for local Jupyter environment (where __file__ may not exist)\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.getcwd())\n",
    "    DATA_FOLDER = os.path.join(NOTEBOOK_DIR, 'data')\n",
    "\n",
    "\n",
    "# Stock universe (add more tickers as needed)\n",
    "STOCK_UNIVERSE = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX', 'AMD', 'CRM']\n",
    "N_STOCKS_AUTO = 4  # Number of stocks to select automatically\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "MODE = 'snapshot'  # 'snapshot' or 'live'\n",
    "\n",
    "# Risk Management\n",
    "MAX_WEIGHT = 0.40  # Maximum allocation per asset (40%)\n",
    "REBALANCE_FREQUENCY = 'M'  # Monthly rebalancing\n",
    "COST_PER_SIDE = 0.001  # 10 bps transaction cost\n",
    "\n",
    "# Benchmark & Analysis\n",
    "BENCHMARK = 'QQQ'  # NASDAQ-100 ETF\n",
    "MIN_DAYS = 500  # Minimum days required for analysis\n",
    "\n",
    "# Monte Carlo & Reproducibility  \n",
    "N_SCENARIOS = 20000  # Number of portfolio scenarios\n",
    "RNG_SEED = 42  # For reproducible results\n",
    "PORTFOLIO_VALUE = 10000  # Initial portfolio value ($10k)\n",
    "\n",
    "# Walk-Forward Analysis Parameters\n",
    "TRAIN_WINDOW = 252  # Training window in days (1 trading year)\n",
    "TEST_WINDOW = 63   # Test window in days (1 trading quarter)\n",
    "RISK_FREE_RATE = 0.02  # Risk-free annualized rate for Sharpe calculation",
    "",
    "# Output Directory Configuration",
    "OUTPUT_DIR = os.getenv('OUTPUT_DIR', os.path.join(NOTEBOOK_DIR, 'outputs'))",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07d8ce-fa16-4aee-b1a1-8441b56ae534",
   "metadata": {},
   "source": [
    "This block ensures reproducibility, sets up logging, and defines small utility functions used throughout the portfolio analysis. It:\n",
    "\n",
    "- Seeds both NumPyâ€™s legacy global RNG and the newer Generator API for consistent random behavior across different NumPy calls.\n",
    "- Configures a module-level logger and prints a run manifest of key parameters.\n",
    "- Provides helper functions for RNG access, annualization of returns/volatility, Sharpe ratio with numerical stability, maximum drawdown, weight normalization, and converting weight arrays to labeled pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63dd32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 17:21:00,814 - INFO - === PORTFOLIO OPTIMIZATION ENHANCED PARAMETERS ===\n",
      "2025-08-23 17:21:00,816 - INFO - Mode: snapshot\n",
      "2025-08-23 17:21:00,817 - INFO - Stock Universe: ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX', 'AMD', 'CRM']\n",
      "2025-08-23 17:21:00,817 - INFO - Auto-selection count: 4\n",
      "2025-08-23 17:21:00,818 - INFO - Date Range: 2020-01-01 to 2024-12-31\n",
      "2025-08-23 17:21:00,819 - INFO - Benchmark: QQQ\n",
      "2025-08-23 17:21:00,819 - INFO - Max Weight per Asset: 40.0%\n",
      "2025-08-23 17:21:00,820 - INFO - Transaction Cost: 10.0 bps per side\n",
      "2025-08-23 17:21:00,820 - INFO - Monte Carlo Scenarios: 20,000\n",
      "2025-08-23 17:21:00,821 - INFO - Random Seed: 42\n",
      "2025-08-23 17:21:00,821 - INFO - Portfolio Value: $10,000\n",
      "2025-08-23 17:21:00,821 - INFO - Training Window: 252 days\n",
      "2025-08-23 17:21:00,821 - INFO - Test Window: 63 days\n",
      "2025-08-23 17:21:00,822 - INFO - Risk Free Rate: 2.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Timestamped, leveled logs\n",
    "logger = logging.getLogger(__name__) # Module-level logger\n",
    "\n",
    "# Utility functions for reproducibility\n",
    "def _rng(seed=None):\n",
    "    \"\"\"Return a reproducible NumPy Generator.\"\"\"\n",
    "    return np.random.default_rng(RNG_SEED if seed is None else seed) # Deterministic generator, optional override\n",
    "\n",
    "def _annualize_return(mean_daily):\n",
    "    return mean_daily * 252.0 # Scale daily mean return to annualized (trading days)\n",
    "\n",
    "def _annualize_vol(std_daily):\n",
    "    return std_daily * np.sqrt(252.0) # Scale daily std to annualized volatility\n",
    "\n",
    "def _sharpe(mean, std, rf=0.0, eps=1e-12):\n",
    "    return (mean - rf) / max(std, eps) # Numerical guard to avoid division by zero when stdâ‰ˆ0\n",
    "\n",
    "def _max_drawdown(nav: pd.Series) -> float:\n",
    "    roll_max = nav.cummax() # Running peak of NAV\n",
    "    dd = nav / roll_max - 1.0 # Drawdown series as percentage from peak\n",
    "    return float(dd.min()) if len(dd) else 0.0 # Worst peak-to-trough loss (returns 0.0 if empty)\n",
    "\n",
    "def _normalize_weights(w: np.ndarray, eps=1e-12):\n",
    "    s = np.sum(w)\n",
    "    if s <= eps:",
    "        return np.zeros_like(w) # Avoid divide-by-zero; return zeros if vector sum is too small\n",
    "    return w / s",
    "\n",
    "def _to_series_weights(w: np.ndarray, tickers):\n",
    "    return pd.Series(w, index=list(tickers), dtype=float) # Attach tickers as index for readability\n",
    "\n",
    "# Log key parameters\n",
    "logger.info(\"=== PORTFOLIO OPTIMIZATION ENHANCED PARAMETERS ===\")\n",
    "logger.info(f\"Mode: {MODE}\")\n",
    "logger.info(f\"Stock Universe: {STOCK_UNIVERSE}\")\n",
    "logger.info(f\"Auto-selection count: {N_STOCKS_AUTO}\")\n",
    "logger.info(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "logger.info(f\"Benchmark: {BENCHMARK}\")\n",
    "logger.info(f\"Max Weight per Asset: {MAX_WEIGHT*100}%\")\n",
    "logger.info(f\"Transaction Cost: {COST_PER_SIDE*10000} bps per side\")\n",
    "logger.info(f\"Monte Carlo Scenarios: {N_SCENARIOS:,}\")\n",
    "logger.info(f\"Random Seed: {RNG_SEED}\")\n",
    "logger.info(f\"Portfolio Value: ${PORTFOLIO_VALUE:,}\")\n",
    "logger.info(f\"Training Window: {TRAIN_WINDOW} days\")\n",
    "logger.info(f\"Test Window: {TEST_WINDOW} days\")\n",
    "logger.info(f\"Risk Free Rate: {RISK_FREE_RATE*100}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6ff72",
   "metadata": {},
   "source": [
    "## DATA INGESTION & VALIDATION FUNCTIONS\n",
    "\n",
    "This block defines functions for loading, validating, and processing stock price data from CSV files. It:\n",
    "\n",
    "- Loads stock price data for a list of tickers from individual CSV files, filtering by a specified date range.\n",
    "- Performs comprehensive data validation, including checks for missing values, duplicate dates, insufficient data, and extreme outliers.\n",
    "- Handles missing data using forward/backward fill and removes stocks with too many gaps or constant prices.\n",
    "- Computes clean daily percentage returns from the validated price data, handling infinite or NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb16fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices_from_csv(tickers, start_date, end_date, data_folder='./', strict=True):\n",
    "    \"\"\"\n",
    "    Load stock prices from CSV files with comprehensive validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tickers : list\n",
    "        List of stock ticker symbols\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str  \n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    data_folder : str\n",
    "        Path to folder containing CSV files\n",
    "    strict : bool\n",
    "        If True, raise errors for data issues; if False, issue warnings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aligned stock prices (Close Price only)\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading price data for {len(tickers)} tickers...\")\n",
    "    \n",
    "    stock_data = {} # per-ticker Close Price series\n",
    "    failed_tickers = [] # collect tickers that fail any step\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Construct file path\n",
    "            file_path = os.path.join(data_folder, f\"{ticker}.csv\")\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "                failed_tickers.append(ticker)\n",
    "                continue\n",
    "            \n",
    "            # Load CSV with date parsing\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "            \n",
    "            # Filter date range\n",
    "            df = df.loc[start_date:end_date]\n",
    "            \n",
    "            if df.empty:\n",
    "                logger.warning(f\"No data for {ticker} in specified date range\")\n",
    "                failed_tickers.append(ticker)\n",
    "                continue\n",
    "            \n",
    "            # Store only Close Price\n",
    "            stock_data[ticker] = df['Close Price']\n",
    "            logger.info(f\"âœ… {ticker}: {len(df)} records loaded\") # keep only the Close Price\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {ticker}: {str(e)}\")\n",
    "            failed_tickers.append(ticker)\n",
    "    \n",
    "    if not stock_data:\n",
    "        raise ValueError(\"No valid stock data loaded!\")\n",
    "    \n",
    "    # Create aligned DataFrame\n",
    "    stock_adj_close = pd.DataFrame(stock_data)\n",
    "    \n",
    "    # Data validation (shape/quality checks, NA handling, ticker filtering)\n",
    "    stock_adj_close = validate_stock_data(stock_adj_close, tickers, failed_tickers, strict)\n",
    "    \n",
    "    logger.info(f\"ðŸ“Š Final dataset: {len(stock_adj_close.columns)} stocks, {len(stock_adj_close)} days\")\n",
    "    return stock_adj_close\n",
    "\n",
    "def validate_stock_data(df, original_tickers, failed_tickers, strict=True):\n",
    "    \"\"\"\n",
    "    Comprehensive data validation and cleaning\n",
    "    \"\"\"\n",
    "    logger.info(\"Performing data validation...\")\n",
    "    \n",
    "    # Report failed tickers\n",
    "    if failed_tickers:\n",
    "        logger.warning(f\"Failed to load: {failed_tickers}\")\n",
    "    \n",
    "    # Check minimum data requirements\n",
    "    if len(df) < MIN_DAYS:\n",
    "        message = f\"Dataset has only {len(df)} days (minimum: {MIN_DAYS})\"\n",
    "        if strict:\n",
    "            raise ValueError(message) # hard fail if strict\n",
    "        else:\n",
    "            logger.warning(message) # proceed with caution if not strict\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.any():\n",
    "        logger.warning(\"Missing data detected:\")\n",
    "        for ticker, missing_count in missing_data[missing_data > 0].items():\n",
    "            logger.warning(f\"  {ticker}: {missing_count} missing values\")\n",
    "        \n",
    "        # Forward fill missing values\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill') # impute gaps by forward/backward fill\n",
    "        logger.info(\"Missing values filled using forward/backward fill\")\n",
    "    \n",
    "    # Check for duplicated dates\n",
    "    if df.index.duplicated().any():\n",
    "        logger.warning(\"Duplicate dates found - removing duplicates\")\n",
    "        df = df[~df.index.duplicated(keep='first')] # keep first occurrence\n",
    "    \n",
    "    # Remove stocks with insufficient data\n",
    "    min_valid_ratio = 0.95  # Require 95% valid data\n",
    "    for ticker in df.columns.copy():\n",
    "        valid_ratio = df[ticker].notna().sum() / len(df)\n",
    "        if valid_ratio < min_valid_ratio:\n",
    "            logger.warning(f\"Removing {ticker}: only {valid_ratio:.1%} valid data\")\n",
    "            df = df.drop(columns=[ticker])\n",
    "    \n",
    "    # Check for extreme outliers and constant series\n",
    "    returns = df.pct_change().dropna()\n",
    "    for ticker in returns.columns.copy():\n",
    "        # Check for constant prices (no variation)\n",
    "        if returns[ticker].std() < 1e-6:\n",
    "            logger.warning(f\"Removing {ticker}: constant price series\")\n",
    "            df = df.drop(columns=[ticker])\n",
    "            continue\n",
    "        \n",
    "        # Check for extreme movements (>50% daily change) for awareness\n",
    "        extreme_moves = (abs(returns[ticker]) > 0.5).sum()\n",
    "        if extreme_moves > 0:\n",
    "            logger.warning(f\"{ticker}: {extreme_moves} extreme daily moves (>50%)\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid stocks remaining after validation!\")\n",
    "    \n",
    "    logger.info(f\"âœ… Validation complete: {len(df.columns)} stocks, {len(df)} days\")\n",
    "    return df\n",
    "\n",
    "def compute_returns(prices_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute daily returns from prices (pct_change), drop first NaNs.\"\"\"\n",
    "    returns = prices_df.pct_change().dropna(how='all')\n",
    "    return returns.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='all') # remove infinities and all-NaN rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced_kpis",
   "metadata": {},
   "source": [
    "## ENHANCED KPI CALCULATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0a67c-9fbe-44bf-ba2a-c3e3bf8d6a9f",
   "metadata": {},
   "source": [
    "This block provides enhanced KPI computation utilities for portfolio analysis. It calculates comprehensive performance and risk metrics (annual return/volatility, Sharpe, max drawdown, Sortino, 95% VaR/CVaR, alpha, beta, information ratio) and lightweight alternatives, plus benchmark-relative KPIs (correlation, tracking error, active return). It assumes daily return series and annualizes appropriately, relying on helper functions (e.g., _annualize_return, _annualize_vol, _sharpe, _max_drawdown) defined elsewhere in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comprehensive_kpis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_kpis(returns: pd.Series, benchmark_returns: pd.Series = None, rf: float = RISK_FREE_RATE) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive KPIs including advanced risk metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Portfolio returns series\n",
    "    benchmark_returns : pd.Series, optional\n",
    "        Benchmark returns for alpha/beta calculation\n",
    "    rf : float\n",
    "        Risk-free rate (annualized)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with comprehensive KPIs\n",
    "    \"\"\"\n",
    "    if len(returns) == 0:\n",
    "        # Graceful fallback: return zeroed metrics if no data\n",
    "        return {\"ann_return\": 0.0, \"ann_vol\": 0.0, \"sharpe\": 0.0, \"max_dd\": 0.0,\n",
    "                \"sortino\": 0.0, \"var_95\": 0.0, \"cvar_95\": 0.0, \"alpha\": 0.0, \"beta\": 0.0, \"info_ratio\": 0.0}\n",
    "    \n",
    "    # Basic metrics\n",
    "    mu_d = float(returns.mean())  # Daily mean return\n",
    "    sd_d = float(returns.std(ddof=0))  # Daily volatility (population std)\n",
    "    ann_r = _annualize_return(mu_d)  # Annualize mean (assumes daily input)\n",
    "    ann_v = _annualize_vol(sd_d)     # Annualize volatility (sqrt(252) convention)\n",
    "    sr = _sharpe(ann_r, ann_v, rf)   # Sharpe uses annualized inputs and annual rf\n",
    "    \n",
    "    # Maximum Drawdown\n",
    "    nav = (1 + returns).cumprod()\n",
    "    mdd = _max_drawdown(nav) # Peak-to-trough % drawdown\n",
    "    \n",
    "    # Sortino Ratio (downside deviation)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_std = float(downside_returns.std(ddof=0)) if len(downside_returns) > 0 else 0.0\n",
    "    sortino = _sharpe(ann_r, _annualize_vol(downside_std), rf) if downside_std > 0 else 0.0 # Uses downside vol\n",
    "    \n",
    "    # Value at Risk (VaR) and Conditional VaR (CVaR) at 95% confidence\n",
    "    var_95 = float(np.percentile(returns, 5)) * np.sqrt(252)  # Left-tail (5th pct), annualized\n",
    "    cvar_95 = float(returns[returns <= np.percentile(returns, 5)].mean()) * np.sqrt(252)  # Tail mean, annualized\n",
    "    \n",
    "    # Alpha, Beta, and Information Ratio vs benchmark\n",
    "    alpha, beta, info_ratio = 0.0, 0.0, 0.0\n",
    "    if benchmark_returns is not None and len(benchmark_returns) > 0:\n",
    "        # Align returns to common days\n",
    "        aligned_idx = returns.index.intersection(benchmark_returns.index)\n",
    "        if len(aligned_idx) > 10:  # Need sufficient data\n",
    "        aligned = pd.concat([returns.rename('p'), benchmark_returns.rename('b')], axis=1).dropna()",
    "        port_aligned = aligned['p']",
    "        bench_aligned = aligned['b']",
    "            \n",
    "            # Beta calculation cov(port, bench) / var(bench)\n",
    "            if bench_aligned.std() > 0:\n",
    "                beta = float(np.cov(port_aligned, bench_aligned)[0, 1] / np.var(bench_aligned))\n",
    "            \n",
    "            # Alpha calculation (annualized CAPM): alpha = Rp - [rf + beta*(Rb - rf)]\n",
    "            bench_ann_return = _annualize_return(bench_aligned.mean())\n",
    "            alpha = ann_r - (rf + beta * (bench_ann_return - rf))\n",
    "            \n",
    "            # Information Ratio = annualized active return / annualized tracking error\n",
    "            active_returns = port_aligned - bench_aligned\n",
    "            tracking_error = _annualize_vol(active_returns.std(ddof=0))\n",
    "            if tracking_error > 0:\n",
    "                info_ratio = _annualize_return(active_returns.mean()) / tracking_error\n",
    "    \n",
    "    return {\n",
    "        \"ann_return\": ann_r,\n",
    "        \"ann_vol\": ann_v,\n",
    "        \"sharpe\": sr,\n",
    "        \"max_dd\": mdd,\n",
    "        \"sortino\": sortino,\n",
    "        \"var_95\": var_95,\n",
    "        \"cvar_95\": cvar_95,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta,\n",
    "        \"info_ratio\": info_ratio,\n",
    "        \"mu_d\": mu_d,\n",
    "        \"vol_d\": sd_d\n",
    "    }\n",
    "\n",
    "def compute_kpis(returns: pd.Series, rf: float = 0.0) -> dict:",
    "    \"\"\"Compute basic KPIs using the comprehensive calculator, then subset.\"\"\"",
    "    k = calculate_comprehensive_kpis(returns=returns, benchmark_returns=None, rf=rf)",
    "    return {k2: k.get(k2, 0.0) for k2 in ('ann_return', 'ann_vol', 'sharpe', 'max_dd')}",
    "",
    "def kpis_vs_benchmark(port_ret: pd.Series, bench_ret: pd.Series) -> dict:\n",
    "    \"\"\"Compute correlation, tracking error, and active return vs benchmark.\"\"\"\n",
    "    idx = port_ret.index.intersection(bench_ret.index)\n",
    "    aligned = pd.concat([port_ret.rename('p'), bench_ret.rename('b')], axis=1).dropna()",
    "    pr, br = aligned['p'], aligned['b']",
    "    corr = float(np.corrcoef(pr, br)[0,1]) if pr.std() > 0 and br.std() > 0 else 0.0 # Only if both var\n",
    "    active = pr - br\n",
    "    te = float(active.std(ddof=0) * np.sqrt(252.0)) # Annualized tracking error\n",
    "    ar = float((_annualize_return(pr.mean()) - _annualize_return(br.mean()))) # Annualized active return\n",
    "    return {\"corr\": corr, \"tracking_error\": te, \"active_return\": ar}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portfolio_optimization",
   "metadata": {},
   "source": [
    "## PORTFOLIO OPTIMIZATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a26ab7-e502-480b-9e8f-dbce97051ee9",
   "metadata": {},
   "source": [
    "This block implements core portfolio optimization utilities used throughout the case study:\n",
    "\n",
    "- Computes average pairwise (off-diagonal) correlation across assets to gauge diversification.\n",
    "- Given asset return series and weights, computes daily portfolio mean/volatility, annualizes them, and derives Sharpe ratio using helper functions.\n",
    "- Optimizes portfolio weights under box constraints and a full-investment constraint using:\n",
    "    - SciPy SLSQP for three objectives: max Sharpe, minimum variance, and risk parity.\n",
    "    - A Monte Carlo fallback that samples feasible weights (Dirichlet), enforces bounds, optionally penalizes turnover, and selects the best according to the chosen objective.\n",
    "- Supports turnover penalties (L1 difference from previous weights), risk-free rate, and reproducible randomness via seed and global settings.\n",
    "\n",
    "**Notes**\n",
    "- returns_df is assumed to be daily returns; annualization is performed via helper functions: _annualize_return, _annualize_vol, _sharpe.\n",
    "- Bounds default to [0, MAX_WEIGHT] and weights sum to sum_to (usually 1.0).\n",
    "- Risk parity minimizes deviations from equal risk contribution.\n",
    "- Helper utilities (_normalize_weights, _to_series_weights, _rng) and globals (MAX_WEIGHT, RISK_FREE_RATE, N_SCENARIOS, logger) are expected from the broader project context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "optimization_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_offdiag_correlation(returns_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Average off-diagonal correlation among assets.\"\"\"\n",
    "    if returns_df.shape[1] <= 1:\n",
    "        return 0.0\n",
    "    corr = returns_df.corr().values\n",
    "    n = corr.shape[0]\n",
    "    return float((corr.sum() - np.trace(corr)) / (n * (n - 1)))\n",
    "\n",
    "def portfolio_stats_from_weights(returns_df: pd.DataFrame, weights: np.ndarray, rf: float = 0.0) -> dict:\n",
    "    \"\"\"Compute mean, vol, sharpe given asset returns and weights.\"\"\"\n",
    "    w = np.asarray(weights).reshape(-1)  # Ensure 1D weight vector\n",
    "    mu = returns_df.mean().values        # Daily expected returns per asset\n",
    "    cov = returns_df.cov().values        # Daily covariance matrix\n",
    "    pmu_d = float(np.dot(w, mu))         # Daily portfolio expected return\n",
    "    pvol_d = float(np.sqrt(max(np.dot(w, cov @ w), 0.0)))  # Daily portfolio volatility (stdev)\n",
    "    ann_r = _annualize_return(pmu_d)     # Annualized return (helper handles frequency assumption)\n",
    "    ann_v = _annualize_vol(pvol_d)       # Annualized volatility\n",
    "    sr = _sharpe(ann_r, ann_v, rf)       # Sharpe ratio using annualized figures and rf\n",
    "    return {\"ann_return\": ann_r, \"ann_vol\": ann_v, \"sharpe\": sr, \"mu_d\": pmu_d, \"vol_d\": pvol_d}\n",
    "\n",
    "def optimize_weights(\n",
    "    returns_df: pd.DataFrame,\n",
    "    objective: str = 'max_sharpe',\n",
    "    bounds: tuple = (0.0, MAX_WEIGHT),\n",
    "    sum_to: float = 1.0,\n",
    "    method: str = 'scipy',\n",
    "    turnover_penalty: float = 0.0,\n",
    "    prev_weights: np.ndarray = None,\n",
    "    risk_free: float = RISK_FREE_RATE,\n",
    "    seed: int = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Optimize portfolio weights under constraints with enhanced objectives\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Asset returns DataFrame\n",
    "    objective : str\n",
    "        'max_sharpe', 'min_var', or 'risk_parity'\n",
    "    bounds : tuple\n",
    "        (lower_bound, upper_bound) for weights\n",
    "    sum_to : float\n",
    "        Target sum of weights (usually 1.0)\n",
    "    method : str\n",
    "        'scipy' or 'mc' (Monte Carlo)\n",
    "    turnover_penalty : float\n",
    "        Penalty for turnover (L1 norm of weight changes)\n",
    "    prev_weights : np.ndarray\n",
    "        Previous weights for turnover calculation\n",
    "    risk_free : float\n",
    "        Risk-free rate\n",
    "    seed : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with optimized weights, KPIs, and method used\n",
    "    \"\"\"\n",
    "    assets = list(returns_df.columns)\n",
    "    n = len(assets)\n",
    "    lb, ub = bounds\n",
    "    lb = max(0.0, float(lb))  # Enforce no shorting if lb < 0 is passed\n",
    "    ub = float(ub)\n",
    "    if ub <= 0:\n",
    "        raise ValueError(\"Upper bound must be > 0\")\n",
    "\n",
    "    mu = returns_df.mean().values\n",
    "    cov = returns_df.cov().values\n",
    "    x0 = np.full(n, sum_to / n, dtype=float)  # Start from equal weights\n",
    "    x0 = np.clip(x0, lb, ub)                  # Respect box constraints\n",
    "    x0 = _normalize_weights(x0) * sum_to      # Re-normalize to the simplex\n",
    "    prev_w = np.zeros(n) if prev_weights is None else np.asarray(prev_weights).reshape(-1)\n",
    "\n",
    "    def obj_sharpe(w):\n",
    "        w = np.asarray(w)\n",
    "        pmu_d = np.dot(w, mu)\n",
    "        pvar = max(np.dot(w, cov @ w), 0.0)\n",
    "        pstd_d = np.sqrt(pvar)\n",
    "        ann_r = _annualize_return(pmu_d)\n",
    "        ann_v = _annualize_vol(pstd_d)\n",
    "        sharpe = _sharpe(ann_r, ann_v, risk_free)\n",
    "        # Minimize negative Sharpe; add L1 turnover penalty vs previous weights\n",
    "        pen = turnover_penalty * np.sum(np.abs(w - prev_w)) if turnover_penalty > 0 else 0.0\n",
    "        return -sharpe + pen\n",
    "\n",
    "    def obj_minvar(w):\n",
    "        w = np.asarray(w)\n",
    "        pen = turnover_penalty * np.sum(np.abs(w - prev_w)) if turnover_penalty > 0 else 0.0\n",
    "        return max(np.dot(w, cov @ w), 0.0) + pen\n",
    "\n",
    "    def obj_risk_parity(w):\n",
    "        \"\"\"Risk Parity objective: minimize sum of squared deviations from equal risk contribution\"\"\"\n",
    "        w = np.asarray(w)\n",
    "        covw = cov @ w\n",
    "        rc = w * covw  # risk contributions using variance\n",
    "        total_var = max(float(w @ covw), 1e-12)\n",
    "        target = total_var / n\n",
    "        pen = turnover_penalty * np.sum(np.abs(w - prev_w)) if turnover_penalty > 0 else 0.0\n",
    "        return float(np.sum((rc - target) ** 2)) + pen\n",
    "\n",
    "    def project_bounds(w):\n",
    "        # Enforce box constraints and renormalize to sum_to\n",
    "        w = np.clip(w, lb, ub)\n",
    "    w = _normalize_weights(w)",
    "    return w * sum_to",
    "    if method == 'scipy':\n",
    "        try:\n",
    "            cons = (\n",
    "                {'type': 'eq', 'fun': lambda w: np.sum(w) - sum_to},\n",
    "            )\n",
    "            bnds = tuple((lb, ub) for _ in range(n))\n",
    "            \n",
    "            if objective == 'max_sharpe':\n",
    "                fun = obj_sharpe\n",
    "            elif objective == 'min_var':\n",
    "                fun = obj_minvar\n",
    "            elif objective == 'risk_parity':\n",
    "                fun = obj_risk_parity\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown objective: {objective}\")\n",
    "            \n",
    "            # SLSQP handles linear equality constraints and box bounds well\n",
    "            res = minimize(fun, x0, method='SLSQP', bounds=bnds, constraints=cons, \n",
    "                         options={'maxiter': 200, 'ftol': 1e-9})\n",
    "            \n",
    "            if res.success:\n",
    "                w_opt = project_bounds(res.x)  # Final guard against numerical drift\n",
    "                stats = portfolio_stats_from_weights(returns_df, w_opt, rf=risk_free)\n",
    "                return {\n",
    "                    'weights': _to_series_weights(w_opt, assets),  # Return as pd.Series with tickers\n",
    "                    'kpis': stats,\n",
    "                    'method_used': 'scipy',\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SciPy optimization failed: {e}, falling back to Monte Carlo\")\n",
    "    \n",
    "    # Monte Carlo fallback\n",
    "    logger.info(\"Using Monte Carlo optimization\")\n",
    "    rng_local = _rng(seed)  # Project helper for reproducible RNG\n",
    "    W = rng_local.dirichlet(alpha=np.ones(n), size=N_SCENARIOS)  # Sample weights on the simplex\n",
    "    mask = (W <= ub + 1e-12).all(axis=1)  # Enforce upper bound per asset\n",
    "    Wf = W[mask]\n",
    "    \n",
    "    if Wf.shape[0] < max(1000, n * 200):\n",
    "        # If too few feasible samples, clip and renormalize rather than discard\n",
    "        Wc = np.clip(W, lb, ub)\n",
    "        Wc = (Wc / Wc.sum(axis=1, keepdims=True)) * sum_to\n",
    "        Wf = Wc\n",
    "    else:\n",
    "        Wf = (Wf / Wf.sum(axis=1, keepdims=True)) * sum_to\n",
    "\n",
    "    if objective == 'max_sharpe':\n",
    "        pmu_d = Wf @ mu\n",
    "        pvar = np.einsum('ij,jk,ik->i', Wf, cov, Wf)  # Vectorized portfolio variance\n",
    "        pstd_d = np.sqrt(np.maximum(pvar, 0.0))\n",
    "        ann_r = _annualize_return(pmu_d)\n",
    "        ann_v = _annualize_vol(pstd_d)\n",
    "        sharpe = (ann_r - risk_free) / np.maximum(ann_v, 1e-12)\n",
    "        if turnover_penalty > 0:\n",
    "            pen = turnover_penalty * np.sum(np.abs(Wf - prev_w.reshape(1, -1)), axis=1)\n",
    "            sharpe = sharpe - pen\n",
    "        idx = int(np.argmax(sharpe))\n",
    "        w_opt = Wf[idx]\n",
    "    elif objective == 'min_var':\n",
    "        pvar = np.einsum('ij,jk,ik->i', Wf, cov, Wf)\n",
    "        if turnover_penalty > 0:\n",
    "            pen = turnover_penalty * np.sum(np.abs(Wf - prev_w.reshape(1, -1)), axis=1)\n",
    "            score = pvar + pen\n",
    "            idx = int(np.argmin(score))\n",
    "        else:\n",
    "            idx = int(np.argmin(pvar))\n",
    "        w_opt = Wf[idx]\n",
    "    elif objective == 'risk_parity':\n",
    "        covW = Wf @ cov  # (m, n)\n",
    "        rc = Wf * covW   # (m, n)\n",
    "        total_var = np.sum(rc, axis=1)\n",
    "        target = (total_var / n).reshape(-1, 1)\n",
    "        score = np.sum((rc - target) ** 2, axis=1)  # Deviation from equal risk contribution\n",
    "        if turnover_penalty > 0:\n",
    "            score = score + turnover_penalty * np.sum(np.abs(Wf - prev_w.reshape(1, -1)), axis=1)\n",
    "        idx = int(np.argmin(score))\n",
    "        w_opt = Wf[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown objective: {objective}\")\n",
    "\n",
    "    w_opt = project_bounds(w_opt)  # Final projection to satisfy bounds/sum\n",
    "    stats = portfolio_stats_from_weights(returns_df, w_opt, rf=risk_free)\n",
    "    return {\n",
    "        'weights': _to_series_weights(w_opt, assets),\n",
    "        'kpis': stats,\n",
    "        'method_used': 'mc',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock_selection",
   "metadata": {},
   "source": [
    "## STOCK SELECTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c86ec-a574-4707-864b-43d0cc9e8034",
   "metadata": {},
   "source": [
    "Selects an optimal subset of assets from the universe using either exhaustive search (for small universes) or a greedy forward-selection heuristic (for larger ones). Over a specified lookback window, it:\n",
    "\n",
    "* Computes returns and evaluates candidate portfolios.  \n",
    "* Optimizes weights for each candidate using a max-Sharpe objective with per-asset weight caps.  \n",
    "* Scores candidates (Sharpe only for exhaustive; Sharpe penalized by correlation for greedy to encourage diversification).  \n",
    "* Returns the chosen tickers, a DataFrame of evaluated combinations with metrics, and the optimal weights for the best selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "selection_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCK SELECTION FUNCTIONS\n",
    "\n",
    "def select_optimal_stocks(\n",
    "    prices_df: pd.DataFrame,\n",
    "    N_stocks: int,\n",
    "    lookback_days: int,\n",
    "    method: str = 'greedy',\n",
    "    seed: int = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Select N_stocks from the universe using advanced selection methodology\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Price data for all assets\n",
    "    N_stocks : int\n",
    "        Number of stocks to select\n",
    "    lookback_days : int\n",
    "        Number of days to look back for analysis\n",
    "    method : str\n",
    "        'exhaustive' for small universes, 'greedy' for larger ones\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with selected tickers, analysis DataFrame, and best weights\n",
    "    \"\"\"\n",
    "    assert N_stocks >= 1, \"N_stocks must be >= 1\"\n",
    "    tickers = list(prices_df.columns)  # Universe tickers\n",
    "    end_idx = prices_df.index[-1]\n",
    "    start_idx = prices_df.index[max(0, len(prices_df) - lookback_days)]  # Start of lookback window\n",
    "    px_lb = prices_df.loc[start_idx:end_idx]  # Price slice for lookback\n",
    "    rets_lb = compute_returns(px_lb)  # Return series aligned to lookback window\n",
    "    combos_rows = []  # Accumulates evaluation metrics per candidate combination\n",
    "\n",
    "    if len(tickers) <= 10 and method == 'exhaustive':\n",
    "        logger.info(f\"Using exhaustive search for {len(tickers)} assets\")\n",
    "        best = None\n",
    "        for combo in combinations(tickers, N_stocks):  # Evaluate all N_stocks-size combos\n",
    "            sub = rets_lb[list(combo)].dropna()  # Require overlapping, non-NA window\n",
    "            if sub.shape[0] < 2:  # Guard against too few observations\n",
    "                continue\n",
    "            opt = optimize_weights(sub, objective='max_sharpe', bounds=(0.0, MAX_WEIGHT), \n",
    "                                 sum_to=1.0, method='scipy', seed=seed)  # Constrained max-Sharpe optimization\n",
    "            k = opt['kpis']\n",
    "            avg_corr = average_offdiag_correlation(sub)  # Cross-asset average correlation\n",
    "            score = k['sharpe']  # Score by Sharpe (no correlation penalty in exhaustive mode)\n",
    "            combos_rows.append({\n",
    "                'combo': combo,\n",
    "                'sharpe': k['sharpe'],\n",
    "                'ann_return': k['ann_return'],\n",
    "                'ann_vol': k['ann_vol'],\n",
    "                'avg_corr': avg_corr,\n",
    "                'score': score,\n",
    "                'weights': opt['weights'],\n",
    "            })\n",
    "            if best is None or score > best['score']:\n",
    "                best = combos_rows[-1]\n",
    "        combos_df = pd.DataFrame(combos_rows)\n",
    "        sel = list(best['combo']) if best else tickers[:N_stocks]\n",
    "        best_weights = best['weights'] if best else pd.Series(np.full(N_stocks, 1.0 / N_stocks), index=sel)\n",
    "        return {'selected_tickers': sel, 'combos_df': combos_df, 'best_weights': best_weights}\n",
    "\n",
    "    # Greedy selection for larger universes\n",
    "    logger.info(f\"Using greedy search for {len(tickers)} assets\")\n",
    "    selected = []\n",
    "    remaining = set(tickers)\n",
    "    best_weights = None\n",
    "    \n",
    "    while len(selected) < N_stocks and remaining:\n",
    "        best_local = None\n",
    "        for cand in list(remaining):\n",
    "            trial = selected + [cand]  # Try adding one new candidate\n",
    "            sub = rets_lb[trial].dropna()  # Work with common, valid return window\n",
    "            if sub.shape[0] < 2:\n",
    "                continue\n",
    "            opt = optimize_weights(sub, objective='max_sharpe', bounds=(0.0, MAX_WEIGHT), \n",
    "                                 sum_to=1.0, method='scipy', seed=seed)\n",
    "            avg_corr = average_offdiag_correlation(sub)\n",
    "            score = opt['kpis']['sharpe'] * (1.0 - avg_corr)  # Penalize high correlation to encourage diversification\n",
    "            row = {\n",
    "                'combo': tuple(trial),\n",
    "                'sharpe': opt['kpis']['sharpe'],\n",
    "                'ann_return': opt['kpis']['ann_return'],\n",
    "                'ann_vol': opt['kpis']['ann_vol'],\n",
    "                'avg_corr': avg_corr,\n",
    "                'score': score,\n",
    "                'weights': opt['weights'],\n",
    "            }\n",
    "            if best_local is None or score > best_local['score']:\n",
    "                best_local = row\n",
    "        \n",
    "        if best_local is None:\n",
    "            break\n",
    "        \n",
    "        selected = list(best_local['combo'])  # Commit the best local addition\n",
    "        best_weights = best_local['weights']  # Corresponding optimal weights for the current selection\n",
    "        remaining = set(tickers) - set(selected)  # Continue with unselected assets\n",
    "        combos_rows.append(best_local)\n",
    "    \n",
    "    combos_df = pd.DataFrame(combos_rows)\n",
    "    if len(selected) > N_stocks:\n",
    "        selected = selected[:N_stocks]\n",
    "        best_weights = best_weights[selected]\n",
    "        best_weights = best_weights / best_weights.sum()  # Re-normalize if truncated\n",
    "    \n",
    "    logger.info(f\"Selected {len(selected)} stocks: {selected}\")\n",
    "    return {'selected_tickers': selected, 'combos_df': combos_df, 'best_weights': best_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation_functions",
   "metadata": {},
   "source": [
    "## SIMULATION & REBALANCING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec695f4-d3cd-452a-92fd-22cb5bfdf0c6",
   "metadata": {},
   "source": [
    "Defines two core utilities for portfolio simulation:\n",
    "\n",
    "- build_weights_schedule creates a time-indexed schedule of constant target weights at a given frequency between two dates.\n",
    "- simulate_rebalance runs a NAV simulation that rebalances to the scheduled target weights at each rebalancing date, applies transaction costs and slippage based on portfolio turnover, and returns NAV, turnover events, cumulative costs, and summary KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "simulation_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_weights_schedule(start_date: pd.Timestamp, end_date: pd.Timestamp, weights: pd.Series, freq: str = 'M') -> pd.DataFrame:\n",
    "    \"\"\"Create a schedule with constant weights at a given frequency between dates (inclusive).\"\"\"\n",
    "    idx = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    if len(idx) == 0:\n",
    "        idx = pd.DatetimeIndex([start_date])  # Ensure at least one date if range is empty\n",
    "    ws = pd.DataFrame(index=idx, columns=weights.index, dtype=float)  # Columns align to tickers in weights\n",
    "    ws.loc[:, :] = weights.values",
    "    return ws",
    "\n",
    "def simulate_rebalance(\n",
    "    prices_df: pd.DataFrame,\n",
    "    weights_schedule: pd.DataFrame,\n",
    "    rebalance_freq: str = 'M',\n",
    "    cost_per_side: float = COST_PER_SIDE,\n",
    "    slippage: float = 0.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simulate NAV with rebalancing at close, applying costs and tracking turnover\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Price data for assets\n",
    "    weights_schedule : pd.DataFrame\n",
    "        Rebalancing schedule (index: dates, columns: tickers)\n",
    "    rebalance_freq : str\n",
    "        Rebalancing frequency\n",
    "    cost_per_side : float\n",
    "        Transaction cost per side\n",
    "    slippage : float\n",
    "        Additional slippage cost\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with NAV series, turnover, costs, and KPIs\n",
    "    \"\"\"\n",
    "    px = prices_df.copy()\n",
    "    px = px[sorted(px.columns)]  # Stable column order for alignment\n",
    "    rets = compute_returns(px)  # Price-to-return conversion (assumed defined elsewhere)\n",
    "    ws = weights_schedule.copy().reindex(rets.index).dropna(how='all').fillna(0.0)  # Align to returns calendar\n",
    "    all_cols = sorted(set(px.columns) | set(ws.columns))  # Union of tickers across data and schedule\n",
    "    rets = rets.reindex(columns=all_cols).fillna(0.0)  # Missing assets -> zero returns\n",
    "    ws = ws.reindex(columns=all_cols).fillna(0.0)      # Missing weights -> zero weight\n",
    "\n",
    "    nav = 1.0  # Start NAV at 1.0 (scaling to dollars happens elsewhere if needed)\n",
    "    current_weights = np.zeros(len(all_cols), dtype=float)  # Current portfolio weights (post-trade)\n",
    "    nav_series = pd.Series(index=rets.index, dtype=float)\n",
    "    turnover_series = pd.Series(0.0, index=rets.index)\n",
    "    cum_cost_value = 0.0\n",
    "    cum_cost_events = 0\n",
    "\n",
    "    for t in rets.index:\n",
    "        r_t = rets.loc[t].values\n",
    "        day_ret = float(np.nansum(current_weights * r_t))  # Portfolio daily return from current weights\n",
    "        nav *= (1.0 + day_ret)\n",
    "\n",
    "        if t in ws.index and not ws.loc[t].isna().all():  # Rebalance on scheduled dates\n",
    "            target = ws.loc[t].values.astype(float)  # Target weights for this rebalance\n",
    "            w_eod_unnorm = current_weights * (1.0 + r_t)  # Drifted weights after price move\n",
    "            s = w_eod_unnorm.sum()\n",
    "            w_eod = w_eod_unnorm / s if s > 0 else current_weights.copy()  # Normalize to 100%\n",
    "            delta = target - w_eod\n",
    "            turnover = float(np.sum(np.abs(delta)))  # Total absolute weight change (two-sided measure)\n",
    "            cost = (cost_per_side + slippage) * turnover * nav  # Costs in NAV terms (per-side + slippage)\n",
    "            nav -= cost\n",
    "            cum_cost_value += float(cost)\n",
    "            cum_cost_events += 1 if turnover > 0 else 0\n",
    "            turnover_series.loc[t] = turnover\n",
    "            current_weights = target.copy()  # Apply target weights post-cost\n",
    "\n",
    "        nav_series.loc[t] = nav  # Record end-of-day NAV\n",
    "\n",
    "    returns_net = nav_series.pct_change().fillna(0.0)  # Net returns after costs\n",
    "    kpis_net = compute_kpis(returns_net)  # Summary performance metrics (assumed defined elsewhere)\n",
    "    return {\n",
    "        'nav': nav_series,\n",
    "        'turnover': turnover_series[turnover_series > 0],\n",
    "        'cum_cost': float(cum_cost_value),\n",
    "        'cum_cost_events': float(cum_cost_events),\n",
    "        'kpis_net': kpis_net,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "walk_forward",
   "metadata": {},
   "source": [
    "## WALK-FORWARD ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26195a5e-00c8-49e3-b615-36ea1dd036f4",
   "metadata": {},
   "source": [
    "Implements the walk-forward validation pipeline for the portfolio optimization case study. For each rolling fold, it:\n",
    "\n",
    "- Splits the historical price series into train and test windows.\n",
    "- Selects a subset of tickers, optimizes portfolio weights on the training window, and carries prior weights for turnover-aware optimization.\n",
    "- Simulates out-of-sample performance with rebalancing and transaction costs.\n",
    "- Collects per-fold in-sample (training) and out-of-sample KPIs and aggregates OOS NAV and KPIs across all folds.\n",
    "- Returns detailed fold results, aggregated out-of-sample KPIs, and the combined OOS NAV series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "walk_forward_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def walk_forward_evaluate(\n",
    "    prices_df: pd.DataFrame,\n",
    "    train_window: int = TRAIN_WINDOW,\n",
    "    test_window: int = TEST_WINDOW,\n",
    "    rebalance_freq: str = REBALANCE_FREQUENCY,\n",
    "    params: dict = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Walk-forward validation pipeline with comprehensive analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Price data for all assets\n",
    "    train_window : int\n",
    "        Training window size in days\n",
    "    test_window : int\n",
    "        Test window size in days\n",
    "    rebalance_freq : str\n",
    "        Rebalancing frequency\n",
    "    params : dict\n",
    "        Parameters for optimization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with fold results, aggregated KPIs, and OOS NAV\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'N_stocks': N_STOCKS_AUTO,\n",
    "            'selection_method': 'exhaustive' if len(prices_df.columns) <= 10 else 'greedy',  # auto method by universe size\n",
    "            'objective': 'max_sharpe',\n",
    "            'opt_method': 'scipy',\n",
    "            'turnover_penalty': 0.0,            # set >0 to penalize weight changes vs previous fold\n",
    "            'cost_per_side': COST_PER_SIDE,     # transaction cost per trade side\n",
    "            'max_weight': MAX_WEIGHT,           # per-asset cap for weight constraints\n",
    "            'seed': RNG_SEED,\n",
    "        }\n",
    "    \n",
    "    px = prices_df.copy()  # avoid mutating caller's DataFrame\n",
    "    dates = px.index       # time axis used for rolling splits\n",
    "    folds = []             # per-fold artifacts (tickers, weights, KPIs)\n",
    "    all_nav = []           # list of OOS NAV segments to concatenate later\n",
    "    i = train_window       # start at first index after initial training window\n",
    "    prev_w = None          # previous fold's weights (for turnover-aware optimization)\n",
    "    rng_seed = params.get('seed', RNG_SEED)\n",
    "    \n",
    "    logger.info(f\"Starting walk-forward analysis: train={train_window}, test={test_window}\")\n",
    "    \n",
    "    # Advance by test_window each iteration; ensure a full test window remains\n",
    "    while i + test_window <= len(dates):\n",
    "        train_start = dates[i - train_window]\n",
    "        train_end = dates[i - 1]\n",
    "        test_start = dates[i]\n",
    "        test_end = dates[i + test_window - 1]\n",
    "        \n",
    "        logger.info(f\"Fold: Train {train_start.date()} to {train_end.date()}, Test {test_start.date()} to {test_end.date()}\")\n",
    "        \n",
    "        px_tr = px.loc[train_start:train_end]\n",
    "\n",
    "        # Stock selection on the training window (method can be exhaustive/greedy)\n",
    "        sel = select_optimal_stocks(\n",
    "            px_tr,\n",
    "            N_stocks=params.get('N_stocks', N_STOCKS_AUTO),\n",
    "            lookback_days=train_window,\n",
    "            method=params.get('selection_method', 'exhaustive' if px_tr.shape[1] <= 10 else 'greedy'),\n",
    "            seed=rng_seed,\n",
    "        )\n",
    "        tickers = sel['selected_tickers']\n",
    "        \n",
    "        # Weight optimization on selected tickers' returns (in-sample)\n",
    "        rets_tr = compute_returns(px_tr[tickers]).dropna()\n",
    "        opt = optimize_weights(\n",
    "            rets_tr,\n",
    "            objective=params.get('objective', 'max_sharpe'),\n",
    "            bounds=(0.0, params.get('max_weight', MAX_WEIGHT)),  # long-only with per-asset cap\n",
    "            sum_to=1.0,                                          # fully invested\n",
    "            method=params.get('opt_method', 'scipy'),\n",
    "            turnover_penalty=params.get('turnover_penalty', 0.0),\n",
    "            prev_weights=prev_w if prev_w is not None else None, # enable turnover penalty if provided\n",
    "            seed=rng_seed,\n",
    "        )\n",
    "        w = opt['weights']\n",
    "        # Persist current weights (ordered by tickers) for next fold's turnover calculations\n",
    "        prev_w = w.reindex(tickers).fillna(0.0).values\n",
    "\n",
    "        # Out-of-sample simulation over the test window with rebalancing and costs\n",
    "        ws = build_weights_schedule(test_start, test_end, w, freq=rebalance_freq)  # schedule of target weights\n",
    "        sim = simulate_rebalance(\n",
    "            px[tickers].loc[dates[0]:test_end],  # include history to allow consistent NAV series construction\n",
    "            ws, \n",
    "            rebalance_freq=rebalance_freq, \n",
    "            cost_per_side=params.get('cost_per_side', COST_PER_SIDE)\n",
    "        )\n",
    "        nav = sim['nav'].loc[test_start:test_end]  # slice OOS segment only\n",
    "        all_nav.append(nav)\n",
    "        \n",
    "        # KPIs: in-sample from optimizer; OOS from simulated NAV\n",
    "        kpis_is = opt['kpis']\n",
    "        kpis_oos = compute_kpis(nav.pct_change().fillna(0.0))\n",
    "        \n",
    "        folds.append({\n",
    "            'train_start': train_start, 'train_end': train_end,\n",
    "            'test_start': test_start, 'test_end': test_end,\n",
    "            'tickers': tickers, 'weights': w,\n",
    "            'kpis_is': kpis_is, 'kpis_oos': kpis_oos,\n",
    "        })\n",
    "        i += test_window  # move to next non-overlapping fold\n",
    "\n",
    "    # Aggregate out-of-sample NAV and compute overall OOS KPIs\n",
    "    nav_oos = pd.concat(all_nav).sort_index() if all_nav else pd.Series(dtype=float)\n",
    "    agg_kpis = compute_kpis(nav_oos.pct_change().fillna(0.0)) if len(nav_oos) else {k: 0.0 for k in ('ann_return','ann_vol','sharpe','max_dd')}\n",
    "    \n",
    "    logger.info(f\"Walk-forward complete: {len(folds)} folds, OOS Sharpe: {agg_kpis.get('sharpe', 0):.3f}\")\n",
    "    \n",
    "    return {'folds': folds, 'agg_kpis_oos': agg_kpis, 'nav_oos': nav_oos}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reporting_functions",
   "metadata": {},
   "source": [
    "## REPORTING & SUMMARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7960a-da38-43f5-973e-36713c118f8d",
   "metadata": {},
   "source": [
    "This block provides two reporting utilities:\n",
    "\n",
    "- generate_exec_summary: Builds a concise, markdown-formatted executive summary from in-sample, out-of-sample, and cost KPIs, including auto-generated recommendations based on thresholds.\n",
    "- load_benchmark_data: Returns a benchmark time series aligned to your universe index. It first tries to load a real benchmark from disk; if unavailable, it synthesizes an equal-weight benchmark from the universe, normalized to a base of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "reporting_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_exec_summary(kpis_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate executive summary with key insights\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kpis_dict : dict\n",
    "        Dictionary with 'is', 'oos', and 'costs' keys\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Markdown formatted executive summary\n",
    "    \"\"\"\n",
    "    # Safely pull nested KPI dicts with empty defaults to avoid KeyError\n",
    "    is_k = kpis_dict.get('is', {})\n",
    "    oos_k = kpis_dict.get('oos', {})\n",
    "    costs = kpis_dict.get('costs', {})\n",
    "    \n",
    "    # Compose markdown lines (returns/vol are assumed as decimals; multiplied by 100 for % display)\n",
    "    # If costs['cost_per_side'] not provided, falls back to global COST_PER_SIDE\n",
    "    lines = [\n",
    "        \"## ðŸ“Š Executive Summary\",\n",
    "        \"\",\n",
    "        f\"**Performance Overview:**\",\n",
    "        f\"- In-sample Sharpe Ratio: {is_k.get('sharpe', 0):.2f} | Out-of-sample Sharpe: {oos_k.get('sharpe', 0):.2f}\",\n",
    "        f\"- Out-of-sample Annual Return: {oos_k.get('ann_return', 0)*100:.2f}% | Volatility: {oos_k.get('ann_vol', 0)*100:.2f}%\",\n",
    "        f\"- Maximum Drawdown: {oos_k.get('max_dd', 0)*100:.2f}%\",\n",
    "        \"\",\n",
    "        f\"**Risk Management:**\",\n",
    "        f\"- Rebalancing events: {costs.get('events', 0):.0f}\",\n",
    "        f\"- Transaction cost per side: {costs.get('cost_per_side', COST_PER_SIDE)*10000:.1f} bps\",\n",
    "        \"\",\n",
    "        f\"**Recommendations:**\",\n",
    "        f\"- {'âœ… Strong' if oos_k.get('sharpe', 0) > 1.0 else 'âš ï¸ Moderate' if oos_k.get('sharpe', 0) > 0.5 else 'âŒ Weak'} risk-adjusted performance\",\n",
    "        f\"- {'âœ… Acceptable' if abs(oos_k.get('max_dd', 0)) < 0.2 else 'âš ï¸ High'} drawdown levels\",\n",
    "        f\"- Monitor tracking error and consider adjusting weight constraints if needed\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def load_benchmark_data(prices_df: pd.DataFrame, benchmark_ticker: str = BENCHMARK) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load benchmark data or create synthetic benchmark\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame\n",
    "        Universe price data\n",
    "    benchmark_ticker : str\n",
    "        Benchmark ticker symbol\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Benchmark price series\n",
    "    \"\"\"\n",
    "    # Try to load benchmark from data folder (expects a CSV at DATA_FOLDER/<ticker>.csv)\n",
    "    benchmark_path = os.path.join(DATA_FOLDER, f\"{benchmark_ticker}.csv\")\n",
    "    \n",
    "    if os.path.exists(benchmark_path):\n",
    "        try:\n",
    "            # Load prices and align to universe index; forward-fill to cover missing dates\n",
    "            benchmark_df = load_prices_from_csv([benchmark_ticker], START_DATE, END_DATE, DATA_FOLDER, strict=False)\n",
    "            if not benchmark_df.empty and benchmark_ticker in benchmark_df.columns:\n",
    "                benchmark_series = benchmark_df[benchmark_ticker].reindex(prices_df.index).fillna(method='ffill')\n",
    "                logger.info(f\"âœ… Loaded {benchmark_ticker} benchmark data\")\n",
    "                return benchmark_series\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {benchmark_ticker}: {e}\")\n",
    "    \n",
    "    # Fallback: build a synthetic equal-weight benchmark from the universe\n",
    "    logger.info(f\"Creating synthetic equal-weight benchmark from universe\")\n",
    "    returns = compute_returns(prices_df)\n",
    "    if returns.empty:\n",
    "        # Preserve index shape; return empty float series if we cannot compute returns\n",
    "        return pd.Series(index=prices_df.index, dtype=float)\n",
    "    \n",
    "    # Equal-weight vector across available columns\n",
    "    w = np.full(len(prices_df.columns), 1.0 / max(1, len(prices_df.columns)))\n",
    "    # Aggregate cross-sectional returns into a single EW return\n",
    "    ew_ret = pd.Series(np.nansum(returns.values * w.reshape(1, -1), axis=1), index=returns.index)\n",
    "    # Convert returns to NAV and normalize to base 100 for comparability\n",
    "    ew_nav = (1 + ew_ret).cumprod()\n",
    "    return ew_nav / ew_nav.iloc[0] * 100  # Normalize to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_functions",
   "metadata": {},
   "source": [
    "## ENHANCED VISUALIZATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde12724-5b12-4d5d-9efd-9516033c5f64",
   "metadata": {},
   "source": [
    "This module defines three Plotly-based visualization functions for the portfolio optimization case study:\n",
    "\n",
    "* create\\_performance\\_dashboard: A 2x2 dashboard showing portfolio vs. benchmark NAV, current allocation, rolling drawdowns, and a KPI comparison bar chart.  \n",
    "* create\\_risk\\_metrics\\_chart: A radar (polar) chart that summarizes key risk metrics such as Sharpe, Sortino, Information Ratio, VaR, CVaR, and Max Drawdown.  \n",
    "* create\\_walk\\_forward\\_analysis\\_chart: A two-panel chart showing the out-of-sample NAV evolution and in-sample vs. out-of-sample Sharpe ratios by fold for walk-forward evaluation.\n",
    "\n",
    "Assumptions and conventions:\n",
    "\n",
    "* NAV inputs are pd.Series with aligned DatetimeIndex.  \n",
    "* KPI values for returns/volatility/drawdowns are decimal fractions (e.g., 0.12 for 12%) and are converted to percentages for plotting.  \n",
    "* Walk-forward results are provided as a dict with folds (list of dicts with kpis\\_is, kpis\\_oos) and nav\\_oos (Series).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "visualization_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_performance_dashboard(portfolio_nav, benchmark_nav, portfolio_kpis, benchmark_kpis, weights_series):\n",
    "    \"\"\"\n",
    "    Create comprehensive performance dashboard with multiple charts\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolio_nav : pd.Series\n",
    "        Portfolio NAV series\n",
    "    benchmark_nav : pd.Series\n",
    "        Benchmark NAV series\n",
    "    portfolio_kpis : dict\n",
    "        Portfolio KPIs\n",
    "    benchmark_kpis : dict\n",
    "        Benchmark KPIs\n",
    "    weights_series : pd.Series\n",
    "        Final portfolio weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Interactive dashboard figure\n",
    "    \"\"\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Portfolio vs Benchmark Performance',\n",
    "            'Portfolio Allocation',\n",
    "            'Rolling Drawdown',\n",
    "            'Performance Metrics Comparison'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "               [{\"secondary_y\": False}, {\"type\": \"bar\"}]],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # 1. Performance comparison\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio_nav.index,\n",
    "            y=portfolio_nav.values,\n",
    "            name='Portfolio',\n",
    "            line=dict(color='#1f77b4', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=benchmark_nav.index,\n",
    "            y=benchmark_nav.values,\n",
    "            name='Benchmark',\n",
    "            line=dict(color='#ff7f0e', width=2, dash='dash')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Portfolio allocation pie chart\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=weights_series.index,\n",
    "            values=weights_series.values,\n",
    "            name=\"Portfolio Weights\",\n",
    "            textinfo='label+percent',\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Rolling drawdown\n",
    "    portfolio_dd = (portfolio_nav / portfolio_nav.cummax() - 1) * 100  # Assumes NAV is cumulative; output in %\n",
    "    benchmark_dd = (benchmark_nav / benchmark_nav.cummax() - 1) * 100  # Same scaling for benchmark\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio_dd.index,\n",
    "            y=portfolio_dd.values,\n",
    "            name='Portfolio DD',\n",
    "            fill='tonexty',\n",
    "            line=dict(color='red', width=1)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=benchmark_dd.index,\n",
    "            y=benchmark_dd.values,\n",
    "            name='Benchmark DD',\n",
    "            line=dict(color='orange', width=1, dash='dash')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Performance metrics comparison\n",
    "    metrics = ['ann_return', 'ann_vol', 'sharpe', 'max_dd']\n",
    "    # Convert return/vol/DD from decimals to percentages for display; Sharpe remains unitless\n",
    "    portfolio_values = [portfolio_kpis.get(m, 0) * (100 if m in ['ann_return', 'ann_vol', 'max_dd'] else 1) for m in metrics]\n",
    "    benchmark_values = [benchmark_kpis.get(m, 0) * (100 if m in ['ann_return', 'ann_vol', 'max_dd'] else 1) for m in metrics]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Annual Return (%)', 'Volatility (%)', 'Sharpe Ratio', 'Max DD (%)'],\n",
    "            y=portfolio_values,\n",
    "            name='Portfolio',\n",
    "            marker_color='#1f77b4'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Annual Return (%)', 'Volatility (%)', 'Sharpe Ratio', 'Max DD (%)'],\n",
    "            y=benchmark_values,\n",
    "            name='Benchmark',\n",
    "            marker_color='#ff7f0e'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'ðŸ“Š Portfolio Optimization Dashboard',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'font': {'size': 20}\n",
    "        },\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        template='plotly_white'  # Consistent clean theme across subplots\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"NAV\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_risk_metrics_chart(comprehensive_kpis):\n",
    "    \"\"\"\n",
    "    Create advanced risk metrics visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    comprehensive_kpis : dict\n",
    "        Dictionary with comprehensive KPIs including VaR, CVaR, etc.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Risk metrics chart\n",
    "    \"\"\"\n",
    "    # Risk metrics data (VaR/CVaR/Max DD expressed as positive percentages for visualization)\n",
    "    risk_metrics = {\n",
    "        'Sharpe Ratio': comprehensive_kpis.get('sharpe', 0),\n",
    "        'Sortino Ratio': comprehensive_kpis.get('sortino', 0),\n",
    "        'Information Ratio': comprehensive_kpis.get('info_ratio', 0),\n",
    "        'VaR (95%)': abs(comprehensive_kpis.get('var_95', 0)) * 100,\n",
    "        'CVaR (95%)': abs(comprehensive_kpis.get('cvar_95', 0)) * 100,\n",
    "        'Max Drawdown': abs(comprehensive_kpis.get('max_dd', 0)) * 100\n",
    "    }\n",
    "    \n",
    "    # Create radar chart for risk metrics\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Normalize values for radar chart (cap ratio-like metrics for readability)\n",
    "    normalized_values = []\n",
    "    for metric, value in risk_metrics.items():\n",
    "        if 'Ratio' in metric:\n",
    "            normalized_values.append(max(0, min(value, 3)))  # Cap ratios at 3\n",
    "        else:\n",
    "            normalized_values.append(value)\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=normalized_values,\n",
    "        theta=list(risk_metrics.keys()),\n",
    "        fill='toself',\n",
    "        name='Risk Metrics',\n",
    "        line_color='rgb(1,90,120)',\n",
    "        fillcolor='rgba(1,90,120,0.2)'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, max(normalized_values) * 1.1]  # Dynamic radial range based on data\n",
    "            )\n",
    "        ),\n",
    "        title={\n",
    "            'text': 'ðŸŽ¯ Advanced Risk Metrics Profile',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'font': {'size': 16}\n",
    "        },\n",
    "        template='plotly_white',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_walk_forward_analysis_chart(walk_forward_results):\n",
    "    \"\"\"\n",
    "    Create walk-forward analysis visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    walk_forward_results : dict\n",
    "        Results from walk_forward_evaluate function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Walk-forward analysis chart\n",
    "    \"\"\"\n",
    "    folds = walk_forward_results['folds']  # List of fold dicts with 'kpis_is' and 'kpis_oos'\n",
    "    nav_oos = walk_forward_results['nav_oos']  # Out-of-sample NAV as a time series\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(\n",
    "            'Out-of-Sample NAV Evolution',\n",
    "            'In-Sample vs Out-of-Sample Sharpe Ratios by Fold'\n",
    "        ),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # 1. OOS NAV evolution\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=nav_oos.index,\n",
    "            y=nav_oos.values,\n",
    "            name='OOS NAV',\n",
    "            line=dict(color='#2E8B57', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. IS vs OOS Sharpe ratios\n",
    "    fold_numbers = list(range(1, len(folds) + 1))  # Sequential fold labels starting at 1\n",
    "    is_sharpe = [fold['kpis_is']['sharpe'] for fold in folds]\n",
    "    oos_sharpe = [fold['kpis_oos']['sharpe'] for fold in folds]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=fold_numbers,\n",
    "            y=is_sharpe,\n",
    "            name='In-Sample Sharpe',\n",
    "            marker_color='#4CAF50',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=fold_numbers,\n",
    "            y=oos_sharpe,\n",
    "            name='Out-of-Sample Sharpe',\n",
    "            marker_color='#FF9800',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'ðŸ”„ Walk-Forward Analysis Results',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'font': {'size': 18}\n",
    "        },\n",
    "        height=700,\n",
    "        showlegend=True,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"NAV\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Fold Number\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Sharpe Ratio\", row=2, col=1)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_execution",
   "metadata": {},
   "source": [
    "## MAIN EXECUTION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_data_loading",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Validation\n",
    "\n",
    "* If running in a notebook/Colab environment, it clones the project repo (only if missing) and points the data path to the repoâ€™s data folder.  \n",
    "* Lists available CSV files to confirm data presence.  \n",
    "* Loads the stock price data for the defined STOCK\\_UNIVERSE and the benchmark series.  \n",
    "* Prints basic diagnostics: number of assets, date coverage, and summary stats (average daily return/volatility and average pairwise correlation).  \n",
    "* Wraps the loading in a try/except to surface failures cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 17:06:32,574 - INFO - Loading price data for 10 tickers...\n",
      "2025-08-22 17:06:32,575 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/AAPL.csv\n",
      "2025-08-22 17:06:32,576 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/MSFT.csv\n",
      "2025-08-22 17:06:32,577 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/GOOGL.csv\n",
      "2025-08-22 17:06:32,577 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/AMZN.csv\n",
      "2025-08-22 17:06:32,578 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/NVDA.csv\n",
      "2025-08-22 17:06:32,578 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/META.csv\n",
      "2025-08-22 17:06:32,579 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/TSLA.csv\n",
      "2025-08-22 17:06:32,579 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/NFLX.csv\n",
      "2025-08-22 17:06:32,580 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/AMD.csv\n",
      "2025-08-22 17:06:32,580 - WARNING - File not found: /Users/maximvf/Library/CloudStorage/GoogleDrive-maxiveloso@gmail.com/Mi unidad/Learning/Business Intelligence Data Analysis (BIDA)/data/CRM.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading stock price data...\n",
      "âŒ Error loading data: No valid stock data loaded!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid stock data loaded!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Loading stock price data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Load stock prices\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     stock_prices \u001b[38;5;241m=\u001b[39m load_prices_from_csv(\n\u001b[1;32m      7\u001b[0m         tickers\u001b[38;5;241m=\u001b[39mSTOCK_UNIVERSE,\n\u001b[1;32m      8\u001b[0m         start_date\u001b[38;5;241m=\u001b[39mSTART_DATE,\n\u001b[1;32m      9\u001b[0m         end_date\u001b[38;5;241m=\u001b[39mEND_DATE,\n\u001b[1;32m     10\u001b[0m         data_folder\u001b[38;5;241m=\u001b[39mDATA_FOLDER,\n\u001b[1;32m     11\u001b[0m         strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Load benchmark data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     benchmark_prices \u001b[38;5;241m=\u001b[39m load_benchmark_data(stock_prices, BENCHMARK)\n",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m, in \u001b[0;36mload_prices_from_csv\u001b[0;34m(tickers, start_date, end_date, data_folder, strict)\u001b[0m\n\u001b[1;32m     56\u001b[0m         failed_tickers\u001b[38;5;241m.\u001b[39mappend(ticker)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stock_data:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid stock data loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Create aligned DataFrame\u001b[39;00m\n\u001b[1;32m     62\u001b[0m stock_adj_close \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stock_data)\n",
      "\u001b[0;31mValueError\u001b[0m: No valid stock data loaded!"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION PIPELINE\n",
    "\n",
    "### Step 1: Data Loading and Validation\n",
    "\n",
    "# Step 1: Load and validate stock price data\n",
    "print(\"ðŸ”„ Cloning repo and locating data...\")\n",
    "\n",
    "# Clone repo if needed\n",
    "REPO_URL = \"https://github.com/maxiveloso/python-portfolio-optimization.git\"\n",
    "REPO_DIR = \"/content/python-portfolio-optimization\"  # Colab-style working directory\n",
    "DATA_FOLDER = os.path.join(REPO_DIR, \"data\")  # CSVs are expected under the repo's data folder\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"ðŸ“¥ Cloning GitHub repo...\")\n",
    "    !git clone -q {REPO_URL} {REPO_DIR}  # Notebook shell command; requires internet access\n",
    "\n",
    "# Confirm data files are present\n",
    "files = os.listdir(DATA_FOLDER)\n",
    "tickers_found = [f.replace(\".csv\", \"\") for f in files if f.endswith(\".csv\")]\n",
    "print(f\"ðŸ“‚ Found {len(tickers_found)} tickers in repo data folder: {', '.join(tickers_found[:10])}...\")\n",
    "\n",
    "# Load data using your existing function\n",
    "try:\n",
    "    stock_prices = load_prices_from_csv(\n",
    "        tickers=STOCK_UNIVERSE,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        data_folder=DATA_FOLDER,\n",
    "        strict=False  # tolerate missing tickers; load whatâ€™s available\n",
    "    )\n",
    "    \n",
    "    benchmark_prices = load_benchmark_data(stock_prices, BENCHMARK)  # aligns to stock index; can synthesize EW if missing\n",
    "    \n",
    "    print(f\"âœ… Data loaded successfully:\")\n",
    "    print(f\"   ðŸ“ˆ Stocks: {len(stock_prices.columns)} assets, {len(stock_prices)} days\")\n",
    "    print(f\"   ðŸŽ¯ Benchmark: {len(benchmark_prices)} days\")\n",
    "    print(f\"   ðŸ“… Date range: {stock_prices.index[0].date()} to {stock_prices.index[-1].date()}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    returns = compute_returns(stock_prices)  # compute daily returns from prices\n",
    "    print(f\"\\nðŸ“Š Universe Statistics:\")\n",
    "    print(f\"   Average daily return: {returns.mean().mean()*100:.3f}%\")\n",
    "    print(f\"   Average daily volatility: {returns.std().mean()*100:.3f}%\")\n",
    "    print(f\"   Average correlation: {average_offdiag_correlation(returns):.3f}\")  # mean of off-diagonal correlations\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_stock_selection",
   "metadata": {},
   "source": [
    "### Step 2: Stock Selection and Portfolio Optimization\n",
    "\n",
    "This pipeline segment selects a subset of stocks from the universe and optimizes portfolio weights across multiple objectives. It:\n",
    "\n",
    "* Chooses a lookback window (up to \\~2 years) and a selection method (exhaustive or greedy) based on universe size.  \n",
    "* Calls a stock selection routine to pick N\\_STOCKS\\_AUTO tickers and reports the chosen list.  \n",
    "* Computes returns for the selected tickers.  \n",
    "* Optimizes portfolio weights for three objectivesâ€”maximum Sharpe, minimum variance, and risk parityâ€”under no-short and max-weight constraints, collecting KPIs for each.  \n",
    "* Selects the Max Sharpe solution as the primary strategy and prints its summary statistics and final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stock_selection_optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:02:00,670 - INFO - Using exhaustive search for 10 assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Performing stock selection and optimization...\n",
      "âœ… Selected 4 stocks: ['NVDA', 'META', 'NFLX', 'CRM']\n",
      "\n",
      "ðŸŽ¯ Optimizing for max_sharpe...\n",
      "   ðŸ“Š max_sharpe: Sharpe=1.182, Return=48.48%, Vol=39.31%\n",
      "\n",
      "ðŸŽ¯ Optimizing for min_var...\n",
      "   ðŸ“Š min_var: Sharpe=0.759, Return=28.63%, Vol=35.11%\n",
      "\n",
      "ðŸŽ¯ Optimizing for risk_parity...\n",
      "   ðŸ“Š risk_parity: Sharpe=1.041, Return=40.10%, Vol=36.61%\n",
      "\n",
      "ðŸ† Primary Strategy (Max Sharpe):\n",
      "   Sharpe Ratio: 1.182\n",
      "   Annual Return: 48.48%\n",
      "   Annual Volatility: 39.31%\n",
      "\n",
      "ðŸ’¼ Portfolio Weights:\n",
      "   NVDA: 40.00%\n",
      "   META: 24.11%\n",
      "   NFLX: 27.62%\n",
      "   CRM: 8.28%\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION PIPELINE\n",
    "\n",
    "### Step 2: Stock Selection and Portfolio Optimization\n",
    "\n",
    "# Step 2: Stock selection and portfolio optimization\n",
    "print(\"ðŸ”„ Performing stock selection and optimization...\")\n",
    "\n",
    "# Stock selection\n",
    "lookback_days = min(504, len(stock_prices))  # 2 years or available data\n",
    "selection_method = 'exhaustive' if len(stock_prices.columns) <= 10 else 'greedy'  # Exhaustive search for small universes; greedy otherwise\n",
    "\n",
    "selection_results = select_optimal_stocks(\n",
    "    prices_df=stock_prices,\n",
    "    N_stocks=N_STOCKS_AUTO,\n",
    "    lookback_days=lookback_days,\n",
    "    method=selection_method,\n",
    "    seed=RNG_SEED\n",
    ")\n",
    "\n",
    "selected_tickers = selection_results['selected_tickers']  # List of chosen tickers from selection stage\n",
    "print(f\"âœ… Selected {len(selected_tickers)} stocks: {selected_tickers}\")\n",
    "\n",
    "# Portfolio optimization with multiple objectives\n",
    "selected_prices = stock_prices[selected_tickers]  # Restrict price data to selected names\n",
    "selected_returns = compute_returns(selected_prices)  # Compute periodic returns used by optimizers\n",
    "\n",
    "# Optimize for different objectives\n",
    "objectives = ['max_sharpe', 'min_var', 'risk_parity']  # Three optimization targets to compare\n",
    "optimization_results = {}\n",
    "\n",
    "for objective in objectives:\n",
    "    print(f\"\\nðŸŽ¯ Optimizing for {objective}...\")\n",
    "    opt_result = optimize_weights(\n",
    "        returns_df=selected_returns,\n",
    "        objective=objective,          # Optimization objective\n",
    "        bounds=(0.0, MAX_WEIGHT),     # Long-only with per-asset max weight cap\n",
    "        sum_to=1.0,                   # Weights must sum to 1 (fully invested)\n",
    "        method='scipy',               # Use SciPy-based solver\n",
    "        risk_free=RISK_FREE_RATE,     # Used for Sharpe-based objective\n",
    "        seed=RNG_SEED                 # Reproducibility for any randomized steps\n",
    "    )\n",
    "    optimization_results[objective] = opt_result\n",
    "    \n",
    "    kpis = opt_result['kpis']\n",
    "    print(f\"   ðŸ“Š {objective}: Sharpe={kpis['sharpe']:.3f}, Return={kpis['ann_return']*100:.2f}%, Vol={kpis['ann_vol']*100:.2f}%\")\n",
    "\n",
    "# Use max_sharpe as primary strategy\n",
    "primary_weights = optimization_results['max_sharpe']['weights']\n",
    "primary_kpis = optimization_results['max_sharpe']['kpis']  # KPI snapshot for the chosen solution\n",
    "\n",
    "print(f\"\\nðŸ† Primary Strategy (Max Sharpe):\")\n",
    "print(f\"   Sharpe Ratio: {primary_kpis['sharpe']:.3f}\")\n",
    "print(f\"   Annual Return: {primary_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   Annual Volatility: {primary_kpis['ann_vol']*100:.2f}%\")\n",
    "print(f\"\\nðŸ’¼ Portfolio Weights:\")\n",
    "for ticker, weight in primary_weights.items():\n",
    "    print(f\"   {ticker}: {weight*100:.2f}%\")  # Final allocation per asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_comprehensive_analysis",
   "metadata": {},
   "source": [
    "### Step 3: Comprehensive KPI Analysis\n",
    "\n",
    "This block computes the portfolioâ€™s daily returns and the benchmarkâ€™s daily returns, then produces a comprehensive set of performance and risk KPIs for the portfolio (and separately for the benchmark as a baseline). It prints a readable summary of annualized return/volatility, Sharpe/Sortino, information ratio, drawdown, VaR/CVaR, and alpha/beta (vs. the benchmark), followed by benchmark-relative diagnostics such as correlation, active return, and tracking error. It uses the global RISK_FREE_RATE and BENCHMARK for risk-adjusted metrics and labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comprehensive_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Calculating comprehensive KPIs...\n",
      "\n",
      "ðŸ“Š Comprehensive Portfolio Analysis:\n",
      "   ðŸ“ˆ Annual Return: 48.48%\n",
      "   ðŸ“‰ Annual Volatility: 39.30%\n",
      "   âš¡ Sharpe Ratio: 1.183\n",
      "   ðŸŽ¯ Sortino Ratio: 1.648\n",
      "   ðŸ“Š Information Ratio: 1.363\n",
      "   ðŸ”» Maximum Drawdown: -62.58%\n",
      "   âš ï¸ VaR (95%): -60.50%\n",
      "   ðŸš¨ CVaR (95%): -88.51%\n",
      "   ðŸ”¢ Alpha: 21.29%\n",
      "   ðŸ“ Beta: 1.357\n",
      "\n",
      "ðŸŽ¯ vs QQQ Benchmark:\n",
      "   ðŸ“Š Correlation: 0.884\n",
      "   ðŸ“ˆ Active Return: 27.91%\n",
      "   ðŸ“‰ Tracking Error: 20.48%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Calculate comprehensive KPIs\n",
    "print(\"ðŸ”„ Calculating comprehensive KPIs...\")\n",
    "\n",
    "# Calculate portfolio returns\n",
    "# Compute daily portfolio returns as the weighted sum of selected asset returns\n",
    "portfolio_returns = (selected_returns * primary_weights).sum(axis=1)\n",
    "\n",
    "# Calculate benchmark returns\n",
    "# Convert benchmark price series to a single-column DataFrame and compute daily returns\n",
    "benchmark_returns = compute_returns(benchmark_prices.to_frame('benchmark'))['benchmark']\n",
    "\n",
    "# Calculate comprehensive KPIs\n",
    "# Full KPI suite for the portfolio (ann. return/vol, Sharpe/Sortino, info ratio, max DD, VaR/CVaR, alpha/beta)\n",
    "# Uses RISK_FREE_RATE for risk-adjusted metrics\n",
    "comprehensive_kpis = calculate_comprehensive_kpis(\n",
    "    returns=portfolio_returns,\n",
    "    benchmark_returns=benchmark_returns,\n",
    "    rf=RISK_FREE_RATE\n",
    ")\n",
    "\n",
    "# Same KPI set for the benchmark (for comparison baselines like alpha/beta)\n",
    "benchmark_kpis = calculate_comprehensive_kpis(\n",
    "    returns=benchmark_returns,\n",
    "    rf=RISK_FREE_RATE\n",
    ")\n",
    "\n",
    "# Display comprehensive metrics\n",
    "# Pretty-print key portfolio metrics (percentages where applicable)\n",
    "print(f\"\\nðŸ“Š Comprehensive Portfolio Analysis:\")\n",
    "print(f\"   Annual Return: {comprehensive_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   Annual Volatility: {comprehensive_kpis['ann_vol']*100:.2f}%\")\n",
    "print(f\"   Sharpe Ratio: {comprehensive_kpis['sharpe']:.3f}\")\n",
    "print(f\"   Sortino Ratio: {comprehensive_kpis['sortino']:.3f}\")\n",
    "print(f\"   Information Ratio: {comprehensive_kpis['info_ratio']:.3f}\")\n",
    "print(f\"   Maximum Drawdown: {comprehensive_kpis['max_dd']*100:.2f}%\")\n",
    "print(f\"   VaR (95%): {comprehensive_kpis['var_95']*100:.2f}%\")\n",
    "print(f\"   CVaR (95%): {comprehensive_kpis['cvar_95']*100:.2f}%\")\n",
    "print(f\"   Alpha: {comprehensive_kpis['alpha']*100:.2f}%\")\n",
    "print(f\"   Beta: {comprehensive_kpis['beta']:.3f}\")\n",
    "\n",
    "# Benchmark comparison\n",
    "# Relative diagnostics vs. benchmark: correlation, active return, tracking error\n",
    "vs_benchmark = kpis_vs_benchmark(portfolio_returns, benchmark_returns)\n",
    "print(f\"\\nðŸŽ¯ vs {BENCHMARK} Benchmark:\")\n",
    "print(f\"   Correlation: {vs_benchmark['corr']:.3f}\")\n",
    "print(f\"   Active Return: {vs_benchmark['active_return']*100:.2f}%\")\n",
    "print(f\"   Tracking Error: {vs_benchmark['tracking_error']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_walk_forward",
   "metadata": {},
   "source": [
    "### Step 4: Walk-Forward Analysis\n",
    "\n",
    "This block performs the walk-forward validation step of the pipeline. It:\n",
    "\n",
    "* Assembles walk-forward parameters (universe size, selection method, objective, optimizer, turnover cost, weight cap, transaction costs, seed).  \n",
    "* Runs walk\\_forward\\_evaluate over rolling train/test windows with a specified rebalance frequency.  \n",
    "* Extracts and prints aggregated out-of-sample (OOS) KPIs and the number of folds.  \n",
    "* Computes and reports average in-sample (IS) vs OOS Sharpe across folds, plus the relative performance decay from IS to OOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "walk_forward_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:02:37,951 - INFO - Starting walk-forward analysis: train=252, test=63\n",
      "2025-08-21 19:02:37,952 - INFO - Fold: Train 2020-01-02 to 2020-12-30, Test 2020-12-31 to 2021-04-01\n",
      "2025-08-21 19:02:37,957 - INFO - Using exhaustive search for 10 assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Performing walk-forward analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:02:38,358 - INFO - Fold: Train 2020-04-02 to 2021-04-01, Test 2021-04-05 to 2021-07-01\n",
      "2025-08-21 19:02:38,360 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:38,782 - INFO - Fold: Train 2020-07-02 to 2021-07-01, Test 2021-07-02 to 2021-09-30\n",
      "2025-08-21 19:02:38,784 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:39,188 - INFO - Fold: Train 2020-10-01 to 2021-09-30, Test 2021-10-01 to 2021-12-30\n",
      "2025-08-21 19:02:39,190 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:39,581 - INFO - Fold: Train 2020-12-31 to 2021-12-30, Test 2021-12-31 to 2022-03-31\n",
      "2025-08-21 19:02:39,583 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:39,990 - INFO - Fold: Train 2021-04-05 to 2022-03-31, Test 2022-04-01 to 2022-07-01\n",
      "2025-08-21 19:02:39,991 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:40,357 - INFO - Fold: Train 2021-07-02 to 2022-07-01, Test 2022-07-05 to 2022-09-30\n",
      "2025-08-21 19:02:40,358 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:40,697 - INFO - Fold: Train 2021-10-01 to 2022-09-30, Test 2022-10-03 to 2022-12-30\n",
      "2025-08-21 19:02:40,699 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:41,057 - INFO - Fold: Train 2021-12-31 to 2022-12-30, Test 2023-01-03 to 2023-04-03\n",
      "2025-08-21 19:02:41,059 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:41,441 - INFO - Fold: Train 2022-04-01 to 2023-04-03, Test 2023-04-04 to 2023-07-05\n",
      "2025-08-21 19:02:41,443 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:42,090 - INFO - Fold: Train 2022-07-05 to 2023-07-05, Test 2023-07-06 to 2023-10-03\n",
      "2025-08-21 19:02:42,092 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:42,491 - INFO - Fold: Train 2022-10-03 to 2023-10-03, Test 2023-10-04 to 2024-01-03\n",
      "2025-08-21 19:02:42,493 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:42,924 - INFO - Fold: Train 2023-01-03 to 2024-01-03, Test 2024-01-04 to 2024-04-04\n",
      "2025-08-21 19:02:42,926 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:43,431 - INFO - Fold: Train 2023-04-04 to 2024-04-04, Test 2024-04-05 to 2024-07-05\n",
      "2025-08-21 19:02:43,433 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:43,861 - INFO - Fold: Train 2023-07-06 to 2024-07-05, Test 2024-07-08 to 2024-10-03\n",
      "2025-08-21 19:02:43,862 - INFO - Using exhaustive search for 10 assets\n",
      "2025-08-21 19:02:44,323 - INFO - Walk-forward complete: 15 folds, OOS Sharpe: 0.238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Walk-Forward Analysis Results (15 folds):\n",
      "   ðŸ“ˆ OOS Annual Return: 9.20%\n",
      "   ðŸ“‰ OOS Annual Volatility: 38.59%\n",
      "   âš¡ OOS Sharpe Ratio: 0.238\n",
      "   ðŸ”» OOS Maximum Drawdown: -36.79%\n",
      "\n",
      "ðŸ“Š Average Performance Across Folds:\n",
      "   ðŸŽ¯ Average IS Sharpe: 1.887\n",
      "   ðŸŽ¯ Average OOS Sharpe: 0.488\n",
      "   ðŸ“‰ Performance Decay: 74.1%\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION PIPELINE\n",
    "\n",
    "### Step 4: Walk-Forward Analysis\n",
    "\n",
    "# Step 4: Walk-forward validation\n",
    "print(\"ðŸ”„ Performing walk-forward analysis...\")\n",
    "\n",
    "# Walk-forward parameters\n",
    "# Note: `selection_method` must be defined upstream (e.g., exhaustive/heuristic/MC); it drives asset selection per fold.\n",
    "wf_params = {\n",
    "    'N_stocks': N_STOCKS_AUTO,\n",
    "    'selection_method': selection_method,\n",
    "    'objective': 'max_sharpe',   # Optimize for Sharpe in each training window\n",
    "    'opt_method': 'scipy',       # Use SciPy optimizer for continuous weights\n",
    "    'turnover_penalty': 0.0,     # No explicit turnover regularization in objective\n",
    "    'cost_per_side': COST_PER_SIDE,  # Transaction cost in bps per buy/sell side\n",
    "    'max_weight': MAX_WEIGHT,    # Per-asset weight cap (risk constraint)\n",
    "    'seed': RNG_SEED,            # For reproducibility across folds\n",
    "}\n",
    "\n",
    "# Perform walk-forward analysis\n",
    "# Expects: `stock_prices` (price history), rolling train/test windows, and rebalance frequency.\n",
    "# Returns: a dict with per-fold results and aggregate KPIs (e.g., keys: 'folds', 'agg_kpis_oos').\n",
    "walk_forward_results = walk_forward_evaluate(\n",
    "    prices_df=stock_prices,\n",
    "    train_window=TRAIN_WINDOW,\n",
    "    test_window=TEST_WINDOW,\n",
    "    rebalance_freq=REBALANCE_FREQUENCY,\n",
    "    params=wf_params\n",
    ")\n",
    "\n",
    "# Display walk-forward results\n",
    "# Aggregate OOS KPIs summarize performance across all test windows; 'folds' holds per-window details.\n",
    "oos_kpis = walk_forward_results['agg_kpis_oos']\n",
    "n_folds = len(walk_forward_results['folds'])\n",
    "\n",
    "print(f\"\\nðŸ”„ Walk-Forward Analysis Results ({n_folds} folds):\")\n",
    "print(f\"   ðŸ“ˆ OOS Annual Return: {oos_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   ðŸ“‰ OOS Annual Volatility: {oos_kpis['ann_vol']*100:.2f}%\")\n",
    "print(f\"   âš¡ OOS Sharpe Ratio: {oos_kpis['sharpe']:.3f}\")\n",
    "print(f\"   ðŸ”» OOS Maximum Drawdown: {oos_kpis['max_dd']*100:.2f}%\")\n",
    "\n",
    "# Calculate average IS vs OOS performance\n",
    "# Compute mean Sharpe across folds for both in-sample (train) and out-of-sample (test) periods.\n",
    "avg_is_sharpe = np.mean([fold['kpis_is']['sharpe'] for fold in walk_forward_results['folds']])\n",
    "avg_oos_sharpe = np.mean([fold['kpis_oos']['sharpe'] for fold in walk_forward_results['folds']])\n",
    "\n",
    "print(f\"\\nðŸ“Š Average Performance Across Folds:\")\n",
    "print(f\"   ðŸŽ¯ Average IS Sharpe: {avg_is_sharpe:.3f}\")\n",
    "print(f\"   ðŸŽ¯ Average OOS Sharpe: {avg_oos_sharpe:.3f}\")\n",
    "print(f\"   ðŸ“‰ Performance Decay: {((avg_is_sharpe - avg_oos_sharpe) / avg_is_sharpe * 100):.1f}%\")  # Note: if avg_is_sharpe == 0, this would divide by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_simulation",
   "metadata": {},
   "source": [
    "### Step 5: Portfolio Simulation with Rebalancing\n",
    "\n",
    "This block it simulates a rebalanced portfolio over time using a predefined weight schedule, applies transaction costs, and computes portfolio performance metrics. It:\n",
    "\n",
    "* Builds a rebalancing schedule from the simulation start to end dates based on final/primary weights and a rebalancing frequency.  \n",
    "* Runs a transaction-cost-aware backtest to get NAV, turnover per rebalance, cumulative costs, and count of cost events.  \n",
    "* Summarizes portfolio results (final value, total return, rebalancing events, transaction costs, average turnover).  \n",
    "* Compares the portfolioâ€™s cumulative performance to a benchmarkâ€™s cumulative return and reports excess return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "portfolio_simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Simulating portfolio with rebalancing...\n",
      "\n",
      "ðŸ’° Portfolio Simulation Results:\n",
      "   ðŸ Final Portfolio Value: $75,498.35\n",
      "   ðŸ“ˆ Total Return: 654.98%\n",
      "   ðŸ”„ Rebalancing Events: 42\n",
      "   ðŸ’¸ Total Transaction Costs: $23.85\n",
      "   ðŸ“Š Average Turnover per Event: 3.48%\n",
      "\n",
      "ðŸŽ¯ vs QQQ Benchmark:\n",
      "   ðŸ“ˆ Portfolio Return: 654.98%\n",
      "   ðŸ“Š Benchmark Return: 136.51%\n",
      "   ðŸ† Excess Return: 518.48%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Portfolio simulation with rebalancing\n",
    "print(\"ðŸ”„ Simulating portfolio with rebalancing...\")\n",
    "\n",
    "# Create rebalancing schedule\n",
    "simulation_start = stock_prices.index[0]\n",
    "simulation_end = stock_prices.index[-1]\n",
    "\n",
    "weights_schedule = build_weights_schedule(\n",
    "    start_date=simulation_start,\n",
    "    end_date=simulation_end,\n",
    "    weights=primary_weights,  # target weights to apply on each scheduled rebalance date\n",
    "    freq=REBALANCE_FREQUENCY  # e.g., 'M' for monthly rebalancing cadence\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "simulation_results = simulate_rebalance(\n",
    "    prices_df=selected_prices,          # price series for selected assets\n",
    "    weights_schedule=weights_schedule,  # dict/series of dates -> weights\n",
    "    rebalance_freq=REBALANCE_FREQUENCY, # controls when trades occur\n",
    "    cost_per_side=COST_PER_SIDE,        # per-side transaction cost (e.g., 10 bps)\n",
    "    slippage=0.0                        # additional execution slippage (set to zero here)\n",
    ")\n",
    "\n",
    "portfolio_nav = simulation_results['nav']               # cumulative NAV series (starts near 1.0)\n",
    "turnover_events = simulation_results['turnover']        # turnover per rebalance event\n",
    "total_costs = simulation_results['cum_cost']            # cumulative transaction costs (NAV terms)\n",
    "cost_events = simulation_results['cum_cost_events']     # number of rebalance events incurring costs\n",
    "\n",
    "# Calculate final portfolio value\n",
    "final_value = portfolio_nav.iloc[-1] * PORTFOLIO_VALUE  # scale NAV by initial capital\n",
    "total_return = (portfolio_nav.iloc[-1] - 1) * 100       # total % return over the simulation\n",
    "\n",
    "print(f\"\\nðŸ’° Portfolio Simulation Results:\")\n",
    "print(f\"   ðŸ Final Portfolio Value: ${final_value:,.2f}\")\n",
    "print(f\"   ðŸ“ˆ Total Return: {total_return:.2f}%\")\n",
    "print(f\"   ðŸ”„ Rebalancing Events: {cost_events:.0f}\")\n",
    "print(f\"   ðŸ’¸ Total Transaction Costs: ${total_costs * PORTFOLIO_VALUE:.2f}\")\n",
    "print(f\"   ðŸ“Š Average Turnover per Event: {turnover_events.mean()*100:.2f}%\")\n",
    "\n",
    "# Benchmark comparison\n",
    "benchmark_nav = (1 + benchmark_returns).cumprod()       # convert returns to NAV-like index via cumprod\n",
    "benchmark_final_value = benchmark_nav.iloc[-1] * PORTFOLIO_VALUE\n",
    "benchmark_total_return = (benchmark_nav.iloc[-1] - 1) * 100\n",
    "\n",
    "print(f\"\\nðŸŽ¯ vs {BENCHMARK} Benchmark:\")\n",
    "print(f\"   ðŸ“ˆ Portfolio Return: {total_return:.2f}%\")\n",
    "print(f\"   ðŸ“Š Benchmark Return: {benchmark_total_return:.2f}%\")\n",
    "print(f\"   ðŸ† Excess Return: {(total_return - benchmark_total_return):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_executive_summary",
   "metadata": {},
   "source": [
    "### Step 6: Executive Summary Generation\n",
    "\n",
    "This step assembles key performance metrics and costs from earlier steps, generates a concise executive summary using a helper function, and prints it neatly. Specifically:\n",
    "\n",
    "- Announces the start of summary generation.\n",
    "- Packages in-sample KPIs, out-of-sample KPIs, and transaction cost info into a single dictionary.\n",
    "- Calls generate_exec_summary to produce a markdown-formatted overview with performance, risk, and recommendations.\n",
    "- Prints the summary between separator lines for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "executive_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generating executive summary...\n",
      "\n",
      "============================================================\n",
      "## ðŸ“Š Executive Summary\n",
      "\n",
      "**Performance Overview:**\n",
      "- In-sample Sharpe Ratio: 1.18 | Out-of-sample Sharpe: 0.24\n",
      "- Out-of-sample Annual Return: 9.20% | Volatility: 38.59%\n",
      "- Maximum Drawdown: -36.79%\n",
      "\n",
      "**Risk Management:**\n",
      "- Rebalancing events: 42\n",
      "- Transaction cost per side: 10.0 bps\n",
      "\n",
      "**Recommendations:**\n",
      "- âŒ Weak risk-adjusted performance\n",
      "- âš ï¸ High drawdown levels\n",
      "- Monitor tracking error and consider adjusting weight constraints if needed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Generate executive summary\n",
    "print(\"ðŸ”„ Generating executive summary...\")\n",
    "\n",
    "# Prepare summary data\n",
    "summary_data = {\n",
    "    'is': comprehensive_kpis,  # In-sample KPIs (dict), computed earlier in the pipeline\n",
    "    'oos': oos_kpis,           # Out-of-sample KPIs (dict), from walk-forward/backtest\n",
    "    'costs': {\n",
    "        'events': cost_events,             # Number of rebalancing/trade events recorded\n",
    "        'cost_per_side': COST_PER_SIDE     # Transaction cost (per side), global parameter\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate executive summary\n",
    "exec_summary = generate_exec_summary(summary_data)  # Returns a markdown-formatted string\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)  # Visual separator\n",
    "print(exec_summary)    # Human-readable executive summary with performance and risk insights\n",
    "print(\"=\"*60)          # Closing separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_visualizations",
   "metadata": {},
   "source": [
    "### Step 7: Enhanced Visualizations\n",
    "\n",
    "This block generates and displays three interactive Plotly visualizations, then tries to save each as standalone HTML files:\n",
    "\n",
    "- A performance dashboard comparing portfolio vs. benchmark, showing allocations and drawdowns.\n",
    "- A risk metrics chart summarizing key KPIs.\n",
    "- A walk-forward analysis chart visualizing out-of-sample results over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "enhanced_visualizations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating enhanced visualizations...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'portfolio_nav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Creating enhanced visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Performance Dashboard\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dashboard_fig \u001b[38;5;241m=\u001b[39m create_performance_dashboard(\n\u001b[0;32m----> 6\u001b[0m     portfolio_nav\u001b[38;5;241m=\u001b[39mportfolio_nav,\n\u001b[1;32m      7\u001b[0m     benchmark_nav\u001b[38;5;241m=\u001b[39mbenchmark_nav,\n\u001b[1;32m      8\u001b[0m     portfolio_kpis\u001b[38;5;241m=\u001b[39mcomprehensive_kpis,\n\u001b[1;32m      9\u001b[0m     benchmark_kpis\u001b[38;5;241m=\u001b[39mbenchmark_kpis,\n\u001b[1;32m     10\u001b[0m     weights_series\u001b[38;5;241m=\u001b[39mprimary_weights\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m dashboard_fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'portfolio_nav' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 7: Create enhanced visualizations (BANS âžœ Performance/DD âžœ Allocation Pie âžœ Diagnostics)\n",
    "print(\"ðŸ”„ Creating enhanced visualizations...\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "\n",
    "# --- Helpers -----------------------------------------------------------------\n",
    "def _to_nav(returns: pd.Series, base: float = 100.0) -> pd.Series:\n",
    "    r = pd.Series(returns).dropna()\n",
    "    if r.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    nav = (1 + r).cumprod()\n",
    "    return nav / nav.iloc[0] * base\n",
    "\n",
    "def _rolling_sharpe(daily_ret: pd.Series, ann_rf: float = 0.0, window: int = 252) -> pd.Series:\n",
    "    r = pd.Series(daily_ret).dropna()\n",
    "    if r.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    rf_d = ann_rf / 252.0\n",
    "    excess = r - rf_d\n",
    "    mu = excess.rolling(window).mean()\n",
    "    sd = excess.rolling(window).std(ddof=0)\n",
    "    return (mu / sd) * np.sqrt(252)\n",
    "\n",
    "def _rolling_vol(daily_ret: pd.Series, window: int = 252) -> pd.Series:\n",
    "    r = pd.Series(daily_ret).dropna()\n",
    "    if r.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    return r.rolling(window).std(ddof=0) * np.sqrt(252)  # annualized\n",
    "\n",
    "def _var_cvar(ret: pd.Series, q: float = 0.05):\n",
    "    r = pd.Series(ret).dropna()\n",
    "    if r.empty:\n",
    "        return np.nan, np.nan\n",
    "    VaR = r.quantile(q)\n",
    "    CVaR = r[r <= VaR].mean()\n",
    "    return VaR, CVaR\n",
    "\n",
    "# --- Ensure NAV/returns are available ----------------------------------------\n",
    "try:\n",
    "    portfolio_nav\n",
    "except NameError:\n",
    "    portfolio_nav = _to_nav(globals().get(\"portfolio_returns\", pd.Series(dtype=float)), base=100.0)\n",
    "\n",
    "try:\n",
    "    benchmark_nav\n",
    "except NameError:\n",
    "    benchmark_nav = _to_nav(globals().get(\"benchmark_returns\", pd.Series(dtype=float)), base=100.0)\n",
    "\n",
    "p_ret = pd.Series(globals().get(\"portfolio_returns\", pd.Series(dtype=float))).dropna()\n",
    "b_ret = pd.Series(globals().get(\"benchmark_returns\", pd.Series(dtype=float))).dropna()\n",
    "\n",
    "# --- 1) BANS: KPI cards (2 rows x 5 cols) -----------------------------------\n",
    "pk = dict(comprehensive_kpis) if 'comprehensive_kpis' in globals() else {}\n",
    "bk = dict(benchmark_kpis) if 'benchmark_kpis' in globals() else {}\n",
    "oos = globals().get(\"oos_kpis\", {}) or {}\n",
    "\n",
    "cards = [\n",
    "    {\"title\": \"Annual Return\",      \"key\": \"ann_return\",  \"pct\": True,  \"ref\": \"bench\"},\n",
    "    {\"title\": \"Annual Volatility\",  \"key\": \"ann_vol\",     \"pct\": True,  \"ref\": \"bench\"},\n",
    "    {\"title\": \"Sharpe Ratio\",       \"key\": \"sharpe\",      \"pct\": False, \"ref\": \"bench\"},\n",
    "    {\"title\": \"Sortino Ratio\",      \"key\": \"sortino\",     \"pct\": False, \"ref\": \"bench\"},\n",
    "    {\"title\": \"Max Drawdown\",       \"key\": \"max_dd\",      \"pct\": True,  \"ref\": \"bench\"},\n",
    "    {\"title\": \"Information Ratio\",  \"key\": \"info_ratio\",  \"pct\": False, \"ref\": 0.0},\n",
    "    {\"title\": \"Alpha\",              \"key\": \"alpha\",       \"pct\": True,  \"ref\": 0.0},\n",
    "    {\"title\": \"Beta\",               \"key\": \"beta\",        \"pct\": False, \"ref\": 1.0},\n",
    "]\n",
    "if oos:\n",
    "    cards.extend([\n",
    "        {\"title\": \"OOS Annual Return\", \"key\": (\"oos\",\"ann_return\"), \"pct\": True,  \"ref\": None},\n",
    "        {\"title\": \"OOS Sharpe\",        \"key\": (\"oos\",\"sharpe\"),     \"pct\": False, \"ref\": None},\n",
    "    ])\n",
    "\n",
    "# Remove duplicate titles (keep first)\n",
    "seen_titles = set()\n",
    "unique_cards = []\n",
    "for c in cards:\n",
    "    if c[\"title\"] not in seen_titles:\n",
    "        unique_cards.append(c)\n",
    "        seen_titles.add(c[\"title\"])\n",
    "cards = unique_cards\n",
    "\n",
    "cols = 5\n",
    "rows = 2\n",
    "# Do NOT pass subplot_titles to avoid duplication (indicator title + subplot title)\n",
    "bans_fig = make_subplots(rows=rows, cols=cols, specs=[[{\"type\": \"domain\"}]*cols for _ in range(rows)])\n",
    "\n",
    "def _resolve_value(card):\n",
    "    key = card[\"key\"]\n",
    "    if isinstance(key, tuple) and key[0] == \"oos\":\n",
    "        return float(oos.get(key[1], np.nan))\n",
    "    return float(pk.get(key, np.nan))\n",
    "\n",
    "def _resolve_reference(card):\n",
    "    ref = card[\"ref\"]\n",
    "    if ref == \"bench\":\n",
    "        return float(bk.get(card[\"key\"], np.nan))\n",
    "    return ref\n",
    "\n",
    "# Fixed fonts for indicators to reduce zoom-sensitivity\n",
    "number_font_size = 44\n",
    "title_font_size = 14\n",
    "delta_font_size = 18\n",
    "\n",
    "for i, card in enumerate(cards, start=1):\n",
    "    r = (i-1)//cols + 1\n",
    "    c = (i-1)%cols + 1\n",
    "    val = _resolve_value(card)\n",
    "    ref = _resolve_reference(card)\n",
    "    pct = card[\"pct\"]\n",
    "    display_val = val * 100.0 if pct and pd.notna(val) else val\n",
    "    display_ref = (ref * 100.0) if (pct and isinstance(ref, (int,float)) and pd.notna(ref)) else ref\n",
    "\n",
    "    delta_dict = None\n",
    "    if ref is not None and pd.notna(display_ref):\n",
    "        delta_dict = dict(reference=display_ref, valueformat=\".2f\", relative=False,\n",
    "                          font=dict(size=delta_font_size))\n",
    "\n",
    "    bans_fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"number+delta\",\n",
    "            value=0.0 if pd.isna(display_val) else display_val,\n",
    "            number=dict(valueformat=\".2f\", suffix=\"%\" if pct else \"\", font=dict(size=number_font_size)),\n",
    "            delta=delta_dict,\n",
    "            title=dict(text=card[\"title\"], font=dict(size=title_font_size))\n",
    "        ),\n",
    "        row=r, col=c\n",
    "    )\n",
    "\n",
    "bans_fig.update_layout(\n",
    "    height=360 * rows,\n",
    "    template=\"plotly_white\",\n",
    "    margin=dict(l=30, r=30, t=80, b=30),\n",
    "    autosize=True\n",
    ")\n",
    "bans_fig.update_xaxes(showgrid=False, zeroline=False)\n",
    "bans_fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "bans_fig.show()\n",
    "try:\n",
    "    bans_fig.write_html(os.path.join(OUTPUT_DIR, 'risk_metrics.html'))\n",
    "except Exception as e:\n",
    "    print(\"Could not write dashboard to {OUTPUT_DIR}/risk_metrics.html â€” file not saved. (\", e, \")\")\n",
    "\n",
    "# --- 2) Performance dashboard: NAV vs Benchmark + Rolling Drawdown -----------\n",
    "perf_fig = make_subplots(\n",
    "    rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.12,\n",
    "    subplot_titles=(\"Portfolio vs Benchmark Performance\", \"Rolling Drawdown\")\n",
    ")\n",
    "\n",
    "# NAV traces (no legend to keep drawdown legend compact)\n",
    "perf_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=portfolio_nav.index, y=portfolio_nav.values,\n",
    "        name=\"Portfolio (NAV)\", line=dict(color=\"#1f77b4\", width=2),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "perf_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=benchmark_nav.index, y=benchmark_nav.values,\n",
    "        name=\"Benchmark (NAV)\", line=dict(color=\"#2ca02c\", width=2, dash=\"dash\"),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "p_dd = (portfolio_nav / portfolio_nav.cummax() - 1) * 100.0\n",
    "b_dd = (benchmark_nav / benchmark_nav.cummax() - 1) * 100.0\n",
    "\n",
    "# Drawdown traces: show legend and place legend inside chart (adjusted to avoid overlap)\n",
    "perf_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=p_dd.index, y=p_dd.values, name=\"Portfolio DD\",\n",
    "        line=dict(color=\"crimson\", width=1), fill=\"tozeroy\", showlegend=True\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "perf_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=b_dd.index, y=b_dd.values, name=\"Benchmark DD\",\n",
    "        # changed color for contrast with crimson\n",
    "        line=dict(color=\"steelblue\", width=1, dash=\"dash\"), showlegend=True\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "perf_fig.update_layout(\n",
    "    height=720,\n",
    "    template=\"plotly_white\",\n",
    "    # Legend placed inside the figure, positioned to avoid overlapping the drawdown title\n",
    "    legend=dict(\n",
    "        x=0.02, y=0.55, xanchor=\"left\", yanchor=\"top\",\n",
    "        bgcolor=\"rgba(255,255,255,0.75)\", bordercolor=\"rgba(0,0,0,0)\", font=dict(size=10)\n",
    "    ),\n",
    "    margin=dict(l=40, r=20, t=60, b=40)\n",
    ")\n",
    "\n",
    "# Y-axis labels for clarity\n",
    "perf_fig.update_yaxes(title_text=\"NAV\", row=1, col=1)\n",
    "perf_fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "\n",
    "perf_fig.update_xaxes(showgrid=False, zeroline=False)\n",
    "perf_fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "perf_fig.show()\n",
    "try:\n",
    "    perf_fig.write_html(os.path.join(OUTPUT_DIR, 'portfolio_dashboard.html'))\n",
    "except Exception as e:\n",
    "    print(\"Could not write dashboard to {OUTPUT_DIR}/portfolio_dashboard.html â€” file not saved. (\", e, \")\")\n",
    "\n",
    "# --- 3 & 4) Diagnostics + Allocation pie in single 2x2 figure ----------------\n",
    "# Prepare weights_series (same logic as before)\n",
    "pw = globals().get(\"primary_weights\", None)\n",
    "if isinstance(pw, pd.Series):\n",
    "    weights_series = pw.dropna()\n",
    "elif pw is not None and \"selected_returns\" in globals():\n",
    "    try:\n",
    "        weights_series = pd.Series(pw, index=selected_returns.columns)\n",
    "    except Exception:\n",
    "        weights_series = pd.Series(dtype=float)\n",
    "else:\n",
    "    weights_series = pd.Series(dtype=float)\n",
    "\n",
    "diag_fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Distribution of Daily Returns\", \"Rolling Sharpe (12m)\", \"Rolling Volatility (12m, ann.)\", \"Portfolio Allocation\"),\n",
    "    horizontal_spacing=0.12, vertical_spacing=0.15,\n",
    "    specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "           [{\"type\": \"xy\"}, {\"type\": \"domain\"}]]\n",
    ")\n",
    "\n",
    "# Distribution with mean & VaR95 (row=1,col=1)\n",
    "mean_r = p_ret.mean()\n",
    "var95, _cvar95 = _var_cvar(p_ret, q=0.05)\n",
    "hist_vals, hist_edges = np.histogram(p_ret.values*100.0, bins=60)\n",
    "y_hist_max = hist_vals.max() if len(hist_vals) else 1\n",
    "\n",
    "diag_fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=0.5*(hist_edges[:-1]+hist_edges[1:]), y=hist_vals,\n",
    "        name=\"Daily Returns (hist)\", marker=dict(color=\"rgba(31,119,180,0.7)\"), showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Vertical lines for mean and VaR as subplot-specific traces (so they do not appear on other subplots)\n",
    "diag_fig.add_trace(\n",
    "    go.Scatter(x=[mean_r*100, mean_r*100], y=[0, y_hist_max*1.05],\n",
    "               mode=\"lines\", line=dict(color=\"red\", dash=\"dash\"), showlegend=False,\n",
    "               hovertemplate=f\"Avg: {mean_r*100:.2f}%<extra></extra>\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "diag_fig.add_trace(\n",
    "    go.Scatter(x=[var95*100, var95*100], y=[0, y_hist_max*1.05],\n",
    "               mode=\"lines\", line=dict(color=\"orange\", dash=\"dash\"), showlegend=False,\n",
    "               hovertemplate=f\"VaR 95%: {var95*100:.2f}%<extra></extra>\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Annotations placed slightly below the subplot title to avoid overlap\n",
    "diag_fig.add_annotation(x=mean_r*100, y=1.02, xref=\"x1\", yref=\"paper\",\n",
    "                        text=f\"Avg: {mean_r*100:.2f}%\", showarrow=False, font=dict(color=\"red\"),\n",
    "                        yanchor=\"bottom\")\n",
    "diag_fig.add_annotation(x=var95*100, y=1.02, xref=\"x1\", yref=\"paper\",\n",
    "                        text=f\"VaR 95%: {var95*100:.2f}%\", showarrow=False, font=dict(color=\"orange\"),\n",
    "                        yanchor=\"bottom\")\n",
    "\n",
    "# Rolling Sharpe (row=1,col=2)\n",
    "rs = _rolling_sharpe(p_ret, ann_rf=globals().get(\"RISK_FREE_RATE\", 0.0), window=252)\n",
    "diag_fig.add_trace(\n",
    "    go.Scatter(x=rs.index, y=rs.values, mode=\"lines\",\n",
    "               line=dict(color=\"green\"), name=\"Rolling Sharpe\",\n",
    "               showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "overall_sharpe = float(pk.get(\"sharpe\", np.nan)) if pk else np.nan\n",
    "# Replace add_hline with a scatter trace to enable hover/tooltips\n",
    "if pd.notna(overall_sharpe) and not rs.empty:\n",
    "    diag_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[rs.index.min(), rs.index.max()],\n",
    "            y=[overall_sharpe, overall_sharpe],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"crimson\", dash=\"dash\"),\n",
    "            name=\"Total Sharpe\",\n",
    "            hovertemplate=f\"Total Sharpe: {overall_sharpe:.2f}<extra></extra>\",\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Add the fixed horizontal at 1.0 also as a scatter for hover\n",
    "diag_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[rs.index.min() if not rs.empty else 0, rs.index.max() if not rs.empty else 1],\n",
    "        y=[1.0, 1.0],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"gray\", dash=\"dash\"),\n",
    "        name=\"Sharpe = 1.0\",\n",
    "        hovertemplate=\"Sharpe = 1.0<extra></extra>\",\n",
    "        showlegend=True\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Rolling Volatility (row=2,col=1) - annualized percent\n",
    "rv = _rolling_vol(p_ret, window=252)\n",
    "diag_fig.add_trace(\n",
    "    go.Scatter(x=rv.index, y=rv.values*100.0, mode=\"lines\",\n",
    "               line=dict(color=\"#1f77b4\"), name=\"Rolling Vol\",\n",
    "               showlegend=True),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Portfolio Allocation Pie (row=2,col=2)\n",
    "alloc_pie = go.Pie(\n",
    "    labels=weights_series.index if not weights_series.empty else [],\n",
    "    values=weights_series.values if not weights_series.empty else [],\n",
    "    textinfo=\"label+percent\", sort=False, hole=0.35, showlegend=False\n",
    ")\n",
    "diag_fig.add_trace(alloc_pie, row=2, col=2)\n",
    "\n",
    "# Y-axis titles for clarity (non-BANS charts)\n",
    "diag_fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "diag_fig.update_yaxes(title_text=\"Sharpe\", row=1, col=2)\n",
    "diag_fig.update_yaxes(title_text=\"Volatility (%)\", row=2, col=1)\n",
    "\n",
    "diag_fig.update_layout(\n",
    "    height=680,\n",
    "    template=\"plotly_white\",\n",
    "    margin=dict(l=40, r=20, t=80, b=40),\n",
    "    legend=dict(x=0.02, y=0.98, xanchor=\"left\", yanchor=\"top\", bgcolor=\"rgba(255,255,255,0.6)\", font=dict(size=10))\n",
    ")\n",
    "\n",
    "diag_fig.update_xaxes(showgrid=False, zeroline=False)\n",
    "diag_fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "diag_fig.show()\n",
    "try:\n",
    "    diag_fig.write_html(os.path.join(OUTPUT_DIR, 'diagnostics.html'))\n",
    "except Exception as e:\n",
    "    print(\"Could not write diagnostics to {OUTPUT_DIR}/diagnostics.html â€” file not saved. (\", e, \")\")\n",
    "\n",
    "print(\"\\nâœ… Visualizations created and saved (order of display):\")\n",
    "    print(f\"   1) ðŸ§­ {OUTPUT_DIR}/risk_metrics.html       (BANS KPI cards - 2 rows x 5 cols)\")",
    "    print(f\"   2) ðŸ“Š {OUTPUT_DIR}/portfolio_dashboard.html (NAV vs Benchmark + Drawdown; DD legend inside chart)\")",
    "    print(f\"   3) ðŸ” {OUTPUT_DIR}/diagnostics.html         (Distribution + Rolling Sharpe + Rolling Vol + Allocation Pie in 2x2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8_export_results",
   "metadata": {},
   "source": [
    "### Step 8: Export Results\n",
    "\n",
    "This block finalizes the portfolio optimization run by exporting all key artifacts and a human-readable summary. It:\n",
    "\n",
    "- Writes final portfolio weights, NAV time series, comprehensive KPIs, and walk-forward metrics to CSV files.\n",
    "- Saves the executive summary as a Markdown file.\n",
    "- Prints a concise run summary with selected assets and top-line performance metrics.\n",
    "- Assumes upstream variables (e.g., primary_weights, portfolio_nav, benchmark_nav, comprehensive_kpis, benchmark_kpis, walk_forward_results, exec_summary, selected_tickers, oos_kpis) are already computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Export results to CSV files\n",
    "print(\"ðŸ”„ Exporting results...\")\n",
    "\n",
    "# Export portfolio weights\n",
    "primary_weights.to_csv(os.path.join(OUTPUT_DIR, 'final_portfolio_weights.csv'), header=['Weight'])  # One-column CSV of final weights\n",
    "\n",
    "# Export NAV series\n",
    "nav_export = pd.DataFrame({\n",
    "    'Date': portfolio_nav.index,\n",
    "    'Portfolio_NAV': portfolio_nav.values,\n",
    "    'Benchmark_NAV': benchmark_nav.reindex(portfolio_nav.index).fillna(method='ffill').values  # Align to portfolio dates; forward-fill gaps\n",
    "})\n",
    "nav_export.to_csv(os.path.join(OUTPUT_DIR, 'nav_series.csv'), index=False)\n",
    "\n",
    "# Export comprehensive KPIs\n",
    "kpis_export = pd.DataFrame({\n",
    "    'Metric': list(comprehensive_kpis.keys()),\n",
    "    'Portfolio': list(comprehensive_kpis.values()),\n",
    "    'Benchmark': [benchmark_kpis.get(k, np.nan) for k in comprehensive_kpis.keys()]  # Map benchmark KPIs by metric; use NaN if missing\n",
    "})\n",
    "kpis_export.to_csv(os.path.join(OUTPUT_DIR, 'comprehensive_kpis.csv'), index=False)\n",
    "\n",
    "# Export walk-forward results\n",
    "wf_export = []\n",
    "for i, fold in enumerate(walk_forward_results['folds']):\n",
    "    wf_export.append({\n",
    "        'Fold': i + 1,\n",
    "        'Train_Start': fold['train_start'],\n",
    "        'Train_End': fold['train_end'],\n",
    "        'Test_Start': fold['test_start'],\n",
    "        'Test_End': fold['test_end'],\n",
    "        'IS_Sharpe': fold['kpis_is']['sharpe'],\n",
    "        'IS_Return': fold['kpis_is']['ann_return'],\n",
    "        'IS_Vol': fold['kpis_is']['ann_vol'],\n",
    "        'OOS_Sharpe': fold['kpis_oos']['sharpe'],\n",
    "        'OOS_Return': fold['kpis_oos']['ann_return'],\n",
    "        'OOS_Vol': fold['kpis_oos']['ann_vol'],\n",
    "        'Selected_Tickers': '|'.join(fold['tickers'])  # Pipe-delimited tickers per fold for readability\n",
    "    })\n",
    "\n",
    "wf_df = pd.DataFrame(wf_export)\n",
    "wf_df.to_csv(os.path.join(OUTPUT_DIR, 'walk_forward_results.csv'), index=False)\n",
    "\n",
    "# Export executive summary\n",
    "with open(os.path.join(OUTPUT_DIR, 'executive_summary.md'), 'w') as f:\n",
    "    f.write(exec_summary)  # Save Markdown-formatted executive summary\n",
    "\n",
    "print(\"\\nâœ… Results exported successfully:\")\n",
    "print(\"   ðŸ’¼ final_portfolio_weights.csv\")\n",
    "print(\"   ðŸ“ˆ nav_series.csv\")\n",
    "print(\"   ðŸ“Š comprehensive_kpis.csv\")\n",
    "print(\"   ðŸ”„ walk_forward_results.csv\")\n",
    "print(\"   ðŸ“ executive_summary.md\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Portfolio optimization analysis complete!\")\n",
    "print(f\"\\nðŸ† Final Results Summary:\")\n",
    "print(f\"   Selected Assets: {', '.join(selected_tickers)}\")\n",
    "print(f\"   Portfolio Sharpe Ratio: {comprehensive_kpis['sharpe']:.3f}\")\n",
    "print(f\"   Annual Return: {comprehensive_kpis['ann_return']*100:.2f}%\")\n",
    "print(f\"   Maximum Drawdown: {comprehensive_kpis['max_dd']*100:.2f}%\")\n",
    "print(f\"   Walk-Forward OOS Sharpe: {oos_kpis['sharpe']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}